
R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # library(tm)
> # library(quanteda)
> # library(data.table)
> # library(textclean)
> # library(stringr)
> # library(textmineR)
> # library(stm)
> # library(textmineR)
> # library(textreuse)
> # library(text2vec)
> # library(pbapply)
> #setwd('bucket_mount/tuolumne/')
> 
> rerun_existing = T
> if(!require(data.table)){install.packages('data.table');require(data.table)}
Loading required package: data.table
> if(!require(tm)){install.packages('tm');require(tm)}
Loading required package: tm
Loading required package: NLP
> if(!require(quanteda)){install.packages('quanteda');require(quanteda)}
Loading required package: quanteda
Package version: 2.1.2
Parallel computing: 2 of 96 threads used.
See https://quanteda.io for tutorials and examples.

Attaching package: â€˜quantedaâ€™

The following objects are masked from â€˜package:tmâ€™:

    as.DocumentTermMatrix, stopwords

The following objects are masked from â€˜package:NLPâ€™:

    meta, meta<-

The following object is masked from â€˜package:utilsâ€™:

    View

> if(!require(textclean)){install.packages('textclean');require(textclean)}
Loading required package: textclean
> if(!require(stringr)){install.packages('stringr');require(stringr)}
Loading required package: stringr
> if(!require(pbapply)){install.packages('pbapply');require(pbapply)}
Loading required package: pbapply
> if(!require(parallel)){install.packages('parallel');require(parallel)}
Loading required package: parallel
> if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
Loading required package: doParallel
Loading required package: foreach
Loading required package: iterators
> if(!require(textreuse)){install.packages('textreuse');require(textreuse)}
Loading required package: textreuse

Attaching package: â€˜textreuseâ€™

The following objects are masked from â€˜package:quantedaâ€™:

    meta, meta<-, tokens

> if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
Loading required package: tidyverse
â”€â”€ [1mAttaching packages[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.0 â”€â”€
[32mâœ“[39m [34mggplot2[39m 3.3.2     [32mâœ“[39m [34mpurrr  [39m 0.3.4
[32mâœ“[39m [34mtibble [39m 3.0.4     [32mâœ“[39m [34mdplyr  [39m 1.0.2
[32mâœ“[39m [34mtidyr  [39m 1.1.2     [32mâœ“[39m [34mforcats[39m 0.5.0
[32mâœ“[39m [34mreadr  [39m 1.4.0     
â”€â”€ [1mConflicts[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
[31mx[39m [34mpurrr[39m::[32maccumulate()[39m masks [34mforeach[39m::accumulate()
[31mx[39m [34mggplot2[39m::[32mannotate()[39m masks [34mNLP[39m::annotate()
[31mx[39m [34mdplyr[39m::[32mbetween()[39m    masks [34mdata.table[39m::between()
[31mx[39m [34mdplyr[39m::[32mfilter()[39m     masks [34mstats[39m::filter()
[31mx[39m [34mdplyr[39m::[32mfirst()[39m      masks [34mdata.table[39m::first()
[31mx[39m [34mdplyr[39m::[32mlag()[39m        masks [34mstats[39m::lag()
[31mx[39m [34mdplyr[39m::[32mlast()[39m       masks [34mdata.table[39m::last()
[31mx[39m [34mreadr[39m::[32mtokenize()[39m   masks [34mtextreuse[39m::tokenize()
[31mx[39m [34mpurrr[39m::[32mtranspose()[39m  masks [34mdata.table[39m::transpose()
[31mx[39m [34mpurrr[39m::[32mwhen()[39m       masks [34mforeach[39m::when()
> #setwd("google_drive/tuolumne/")
> #setDTthreads(3)
> #getDTthreads()
> mcores = floor(detectCores() - 15 )
> options("mc.cores" = floor(detectCores() - 15))
> 
> #table(projects$AGENCY,projects$PROJECT_TYPE)
> projects = fread('../bucket_mount/tuolumne/scratch/boilerplate/project_candidates_eis_only.csv')
> projects = projects[Document=='Final',]
> projects = projects[grepl('^201[3-9]|^2020',PROJECT_ID),]
> documents = fread( '../bucket_mount/tuolumne/scratch/boilerplate/document_candidates_eis_only.csv')
> documents = documents[PROJECT_ID %in% projects$PROJECT_ID,]
> scratch_loc = '../bucket_mount/tuolumne/scratch/boilerplate/hash_candidates/'
> 
> 
> minhash <- minhash_generator(n = 240, seed = 40)
> progress_bars = T
> gc()
          used  (Mb) gc trigger  (Mb) max used  (Mb)
Ncells 2133686 114.0    4351737 232.5  2615374 139.7
Vcells 3898619  29.8    8388608  64.0  4779112  36.5
> cluster = makeCluster(mcores)
> registerDoParallel(cl = cluster,cores = mcores)
> parallel::clusterEvalQ(cluster,'require(data.table)')
[[1]]
[1] "require(data.table)"

[[2]]
[1] "require(data.table)"

[[3]]
[1] "require(data.table)"

[[4]]
[1] "require(data.table)"

[[5]]
[1] "require(data.table)"

[[6]]
[1] "require(data.table)"

[[7]]
[1] "require(data.table)"

[[8]]
[1] "require(data.table)"

[[9]]
[1] "require(data.table)"

[[10]]
[1] "require(data.table)"

[[11]]
[1] "require(data.table)"

[[12]]
[1] "require(data.table)"

[[13]]
[1] "require(data.table)"

[[14]]
[1] "require(data.table)"

[[15]]
[1] "require(data.table)"

[[16]]
[1] "require(data.table)"

[[17]]
[1] "require(data.table)"

[[18]]
[1] "require(data.table)"

[[19]]
[1] "require(data.table)"

[[20]]
[1] "require(data.table)"

[[21]]
[1] "require(data.table)"

[[22]]
[1] "require(data.table)"

[[23]]
[1] "require(data.table)"

[[24]]
[1] "require(data.table)"

[[25]]
[1] "require(data.table)"

[[26]]
[1] "require(data.table)"

[[27]]
[1] "require(data.table)"

[[28]]
[1] "require(data.table)"

[[29]]
[1] "require(data.table)"

[[30]]
[1] "require(data.table)"

[[31]]
[1] "require(data.table)"

[[32]]
[1] "require(data.table)"

[[33]]
[1] "require(data.table)"

[[34]]
[1] "require(data.table)"

[[35]]
[1] "require(data.table)"

[[36]]
[1] "require(data.table)"

[[37]]
[1] "require(data.table)"

[[38]]
[1] "require(data.table)"

[[39]]
[1] "require(data.table)"

[[40]]
[1] "require(data.table)"

[[41]]
[1] "require(data.table)"

[[42]]
[1] "require(data.table)"

[[43]]
[1] "require(data.table)"

[[44]]
[1] "require(data.table)"

[[45]]
[1] "require(data.table)"

[[46]]
[1] "require(data.table)"

[[47]]
[1] "require(data.table)"

[[48]]
[1] "require(data.table)"

[[49]]
[1] "require(data.table)"

[[50]]
[1] "require(data.table)"

[[51]]
[1] "require(data.table)"

[[52]]
[1] "require(data.table)"

[[53]]
[1] "require(data.table)"

[[54]]
[1] "require(data.table)"

[[55]]
[1] "require(data.table)"

[[56]]
[1] "require(data.table)"

[[57]]
[1] "require(data.table)"

[[58]]
[1] "require(data.table)"

[[59]]
[1] "require(data.table)"

[[60]]
[1] "require(data.table)"

[[61]]
[1] "require(data.table)"

[[62]]
[1] "require(data.table)"

[[63]]
[1] "require(data.table)"

[[64]]
[1] "require(data.table)"

[[65]]
[1] "require(data.table)"

[[66]]
[1] "require(data.table)"

[[67]]
[1] "require(data.table)"

[[68]]
[1] "require(data.table)"

[[69]]
[1] "require(data.table)"

[[70]]
[1] "require(data.table)"

[[71]]
[1] "require(data.table)"

[[72]]
[1] "require(data.table)"

[[73]]
[1] "require(data.table)"

[[74]]
[1] "require(data.table)"

[[75]]
[1] "require(data.table)"

[[76]]
[1] "require(data.table)"

[[77]]
[1] "require(data.table)"

[[78]]
[1] "require(data.table)"

[[79]]
[1] "require(data.table)"

[[80]]
[1] "require(data.table)"

[[81]]
[1] "require(data.table)"

> parallel::clusterEvalQ(cluster,'require(textreuse)')
[[1]]
[1] "require(textreuse)"

[[2]]
[1] "require(textreuse)"

[[3]]
[1] "require(textreuse)"

[[4]]
[1] "require(textreuse)"

[[5]]
[1] "require(textreuse)"

[[6]]
[1] "require(textreuse)"

[[7]]
[1] "require(textreuse)"

[[8]]
[1] "require(textreuse)"

[[9]]
[1] "require(textreuse)"

[[10]]
[1] "require(textreuse)"

[[11]]
[1] "require(textreuse)"

[[12]]
[1] "require(textreuse)"

[[13]]
[1] "require(textreuse)"

[[14]]
[1] "require(textreuse)"

[[15]]
[1] "require(textreuse)"

[[16]]
[1] "require(textreuse)"

[[17]]
[1] "require(textreuse)"

[[18]]
[1] "require(textreuse)"

[[19]]
[1] "require(textreuse)"

[[20]]
[1] "require(textreuse)"

[[21]]
[1] "require(textreuse)"

[[22]]
[1] "require(textreuse)"

[[23]]
[1] "require(textreuse)"

[[24]]
[1] "require(textreuse)"

[[25]]
[1] "require(textreuse)"

[[26]]
[1] "require(textreuse)"

[[27]]
[1] "require(textreuse)"

[[28]]
[1] "require(textreuse)"

[[29]]
[1] "require(textreuse)"

[[30]]
[1] "require(textreuse)"

[[31]]
[1] "require(textreuse)"

[[32]]
[1] "require(textreuse)"

[[33]]
[1] "require(textreuse)"

[[34]]
[1] "require(textreuse)"

[[35]]
[1] "require(textreuse)"

[[36]]
[1] "require(textreuse)"

[[37]]
[1] "require(textreuse)"

[[38]]
[1] "require(textreuse)"

[[39]]
[1] "require(textreuse)"

[[40]]
[1] "require(textreuse)"

[[41]]
[1] "require(textreuse)"

[[42]]
[1] "require(textreuse)"

[[43]]
[1] "require(textreuse)"

[[44]]
[1] "require(textreuse)"

[[45]]
[1] "require(textreuse)"

[[46]]
[1] "require(textreuse)"

[[47]]
[1] "require(textreuse)"

[[48]]
[1] "require(textreuse)"

[[49]]
[1] "require(textreuse)"

[[50]]
[1] "require(textreuse)"

[[51]]
[1] "require(textreuse)"

[[52]]
[1] "require(textreuse)"

[[53]]
[1] "require(textreuse)"

[[54]]
[1] "require(textreuse)"

[[55]]
[1] "require(textreuse)"

[[56]]
[1] "require(textreuse)"

[[57]]
[1] "require(textreuse)"

[[58]]
[1] "require(textreuse)"

[[59]]
[1] "require(textreuse)"

[[60]]
[1] "require(textreuse)"

[[61]]
[1] "require(textreuse)"

[[62]]
[1] "require(textreuse)"

[[63]]
[1] "require(textreuse)"

[[64]]
[1] "require(textreuse)"

[[65]]
[1] "require(textreuse)"

[[66]]
[1] "require(textreuse)"

[[67]]
[1] "require(textreuse)"

[[68]]
[1] "require(textreuse)"

[[69]]
[1] "require(textreuse)"

[[70]]
[1] "require(textreuse)"

[[71]]
[1] "require(textreuse)"

[[72]]
[1] "require(textreuse)"

[[73]]
[1] "require(textreuse)"

[[74]]
[1] "require(textreuse)"

[[75]]
[1] "require(textreuse)"

[[76]]
[1] "require(textreuse)"

[[77]]
[1] "require(textreuse)"

[[78]]
[1] "require(textreuse)"

[[79]]
[1] "require(textreuse)"

[[80]]
[1] "require(textreuse)"

[[81]]
[1] "require(textreuse)"

> #hash_file = paste0('../bucket_mount/big_eis_text.rds')
> full_tlist <- readRDS('../bucket_mount/tuolumne/scratch/boilerplate/big_text_files/big_eis_text.rds')
> 
> chars = nchar(full_tlist$text)
> periods = stringr::str_count(full_tlist$text,"\\.")
> numbers = stringr::str_count(full_tlist$text,"[0-9]")
> caps = stringr::str_count(full_tlist$text,'[A-Z]')
> tildes = stringr::str_count(full_tlist$text,'~')
> quotes = stringr::str_count(full_tlist$text,'\\"')
> spaces = stringr::str_count(full_tlist$text,'\\s')
> 
> cut = 0.1
> full_tlist  = full_tlist[chars>400&{periods/chars}<cut&{quotes/chars}<cut&{tildes/chars}<cut&{numbers/chars}<cut&{caps/chars}<cut&{spaces/chars}<{cut*2},]
> 
> #####
> 
> hash_file = paste0(scratch_loc,'eis_page_hashes.rds')
> flist = as.character(full_tlist$text)
> names(flist) <- paste0(full_tlist$File,'_',full_tlist$Page)
> 
> eis_corpus =  TextReuseCorpus(text = flist,
+                               meta = list(File = full_tlist$File,Page = full_tlist$Page),
+                               tokenizer = tokenize_ngrams, n = 10,
+                               minhash_func = minhash, keep_tokens = TRUE,
+                               progress = progress_bars,skip_short = T)
Loading, tokenizing, and hashing 779,142 documents.
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells  283342570 15132.2  461850117 24665.5  295034156 15756.6
Vcells 4215258774 32159.9 6302442099 48083.9 4376573677 33390.7
> saveRDS(eis_corpus,'../bucket_mount/tuolumne/scratch/eis_page_corp_scratch.rds')
> 
> #eis_corpus = readRDS('../bucket_mount/tuolumne/scratch/eis_page_corp_scratch.rds')
> #file.exists('../bucket_mount/tuolumne/scratch/eis_page_corp_scratch.rds')
> split_corpus_ntiles = dplyr::ntile(x = seq(eis_corpus),n = mcores*10)
> split_corpus = split(eis_corpus,split_corpus_ntiles)
> 
> split_buckets = foreach(x = split_corpus) %dopar% {textreuse::lsh(x,bands = 40)}
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells  313515496 16743.6  461850117 24665.5  316396817 16897.5
Vcells 4520482809 34488.6 7563010518 57701.2 4543645819 34665.3
> 
> while(any(sapply(split_buckets,is.null))){
+   null_fails = which(sapply(split_buckets,is.null))
+   split_buckets[null_fails] <- pblapply(null_fails,function(x) lsh(split_corpus[[x]],bands = 40,progress = F),cl = 5)
+ }
> 
> eis_buckets = do.call(rbind,split_buckets)
> saveRDS(eis_buckets,'../bucket_mount/tuolumne/scratch/eis_page_buckets_scratch.rds')
> 
> #eis_buckets = readRDS('../bucket_mount/tuolumne/scratch/eis_page_buckets_scratch.rds')
> eis_candidates <- lsh_candidates(buckets = eis_buckets)
> require(dplyr)
> candidate_splits = split(eis_candidates,ntile(1:nrow(eis_candidates),n = nrow(eis_candidates) %/% 10000))
> gc()
             used    (Mb) gc trigger    (Mb)   max used    (Mb)
Ncells  313568381 16746.4  461850117 24665.5  344441240 18395.2
Vcells 4593921100 35048.9 7563010518 57701.2 6850934775 52268.5
> 
> #corpus_meta = eis_corpus$meta
> #corpus_meta$names <- names(eis_corpus)[!names(eis_corpus)%in%skipped(eis_corpus)]
> require(doParallel)
> stopImplicitCluster()
> 
> score_list = foreach(i = candidate_splits) %dopar% {
+   send_names = unique(c(i$a,i$b))
+   send_text = flist[send_names]
+   score_list = mapply(function(aa,bb) textreuse::align_local(a = aa,b=bb)$score,aa = send_text[i$a],bb = send_text[i$b])
+   data.table::data.table(a = i$a, b= i$b,score = score_list)
+ }
> 
> 
> score_dt = rbindlist(score_list)
> 
> saveRDS(score_dt,'../bucket_mount/tuolumne/scratch/eis_page_scores_scratch.rds')
> # 
> # if(rerun_existing|!file.exists(hash_file)){
> # 
> #   tlist = foreach(i = seq_along(flist)) %dopar% {
> #     t1 = readLines(paste0(tokpars,flist[i]))
> #     t1}
> #   names(tlist)<- flist
> #  
> #   full_tlist = unlist(tlist)
> #   rm(tlist)
> #   gc()
> #   #saveRDS(full_tlist,'scratch/paragraph_list.rds')
> #   full_tlist <- readRDS('../bucket_mount/paragraph_list.rds')
> #   eis_corpus =  TextReuseCorpus(text = full_tlist,
> #                                 meta = list(ID = names(full_tlist)),
> #                                 tokenizer = tokenize_ngrams, n = 10,
> #                                 minhash_func = minhash, keep_tokens = TRUE,
> #                                 progress = progress_bars,skip_short = T)
> #   saveRDS(eis_corpus,'../bucket_mount/scratch/eis_corp_scratch.rds')
> #   eis_buckets <- lsh(eis_corpus, bands = 80, progress = progress_bars)
> #   eis_candidates <- lsh_candidates(eis_buckets)
> #   
> #   
> #   
> #   
> #   eis_corpus =  TextReuseCorpus(text = text_set[keep],
> #                                 meta = list(Project_File_Par = names(text_set)[keep],PROJECT_ID = str_remove(names(text_set)[keep],'--.*')),
> #                                 tokenizer = tokenize_ngrams, n = 10,
> #                                 minhash_func = minhash, keep_tokens = TRUE,
> #                                 progress = progress_bars,skip_short = T)
> #   eis_buckets <- lsh(eis_corpus, bands = 40, progress = progress_bars)
> #   eis_candidates <- lsh_candidates(eis_buckets)
> #   
> #   
> #   
> #     #t2 = t1$text;names(t2) <- t1$Project_File_Par
> #     t2}
> #   
> #   
> #   
> #   eis_flist = flist[gsub('--','',str_extract(flist,'--.+\\.(PDF|pdf)')) %in% documents$FILE_NAME[documents$PROJECT_TYPE=='EIS']]
> #   floc = 'input/filtered_text_files/'
> #   fls = paste0(floc,eis_flist)
> #   tlist = foreach(i = seq_along(fls)) %dopar% {
> #     t1 = data.table::fread(fls[i],sep ='\t');
> #     t2 = t1$text;names(t2) <- t1$Project_File_Par
> #     t2}
> #   text_set = unlist(tlist)
> #   keep = which(nchar(text_set)>=750)
> #   
> #   eis_corpus =  TextReuseCorpus(text = text_set[keep],
> #                                 meta = list(Project_File_Par = names(text_set)[keep],PROJECT_ID = str_remove(names(text_set)[keep],'--.*')),
> #                                 tokenizer = tokenize_ngrams, n = 10,
> #                                 minhash_func = minhash, keep_tokens = TRUE,
> #                                 progress = progress_bars,skip_short = T)
> #   eis_buckets <- lsh(eis_corpus, bands = 40, progress = progress_bars)
> #   eis_candidates <- lsh_candidates(eis_buckets)
> #   #eis_candidates$a = text_names[keep][as.numeric(str_extract(eis_candidates$a,'[0-9]{1,}'))]
> #   #eis_candidates$b = text_names[keep][as.numeric(str_extract(eis_candidates$b,'[0-9]{1,}'))]
> #   saveRDS(eis_candidates,hash_file)
> # }
> # 
> 
> 
> 
