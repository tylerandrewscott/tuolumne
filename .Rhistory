(gg_threshold = ggplot() + geom_vline(xintercept = 3,lty = 2,col = 'grey50') + geom_path(aes(x = x_vals,y= dns)) + geom_point(aes(x = x_vals,y= dns)) + theme_bw() +
xlab('threshold for y_ij = 1') + ylab('graph density') + ggtitle('Graph density vs. threshold for shared pages'))
ggsave(plot = gg_threshold,filename = 'boilerplate_project/output/figures/FigureA3_tiethreshold_density.tiff',height = 4,width = 6,units = 'in',dpi = 500)
final_edge_list = edge_counts[N>=3,]
eis_net <- eis_init
network::add.edges.network(eis_net,tail = match( final_edge_list$a_id,network.vertex.names(eis_net)),
head = match( final_edge_list$b_id,network.vertex.names(eis_net)),names.eval = 'Score_Count',
vals.eval =  final_edge_list$N)
eis_net %v% 'Agency' <- as.character(projects$AGENCY[match(network.vertex.names(eis_net),projects$EIS.Number)])
eis_net %v% 'Agency_Short' <- as.character(projects$AGENCY_SHORT[match(network.vertex.names(eis_net),projects$EIS.Number)])
eis_net %v% 'Fed_Reg_Date' <- decimal_date(mdy(projects$Federal.Register.Date[match(network.vertex.names(eis_net),projects$EIS.Number)]))
eis_net %v% 'Year' <- (eis_net %v% 'Fed_Reg_Date')-2013
eis_net %v% 'EIS' <- network.vertex.names(eis_net)
eis_net %v% 'ln_Pages_Hashed' <- log(page_counts$total_pages[match(eis_net%v%'vertex.names',page_counts$EIS.Number)])
eis_net %v% 'NEPA_litigation_per_FEIS_2001_2012' = ltd_total$litigation_per_eis[match(eis_net %v% 'Agency_Short', ltd_total$recode)]
eis_net %v% 'Ideo_Rating' <- projects$ideo_rating[match(network.vertex.names(eis_net),projects$EIS.Number)]
eis_net %v% 'Skill_Rating' <- projects$skills_rating[match(network.vertex.names(eis_net),projects$EIS.Number)]
# eis_net %v% 'sqrt_NEPA_litigation_2001_2012'  <- sqrt(eis_net %v% 'NEPA_litigation_2001_2012' )
eis_net %v% 'Consultant' <- network.vertex.names(eis_net) %in% consults$EIS.Number
eis_distance_matrix = abs(dist_mat[rownames(dist_mat) %in% {eis_net %v% 'vertex.names'},colnames(dist_mat) %in% {eis_net %v% 'vertex.names'}])
readab = readRDS('boilerplate_project/data_products/readability_scores_by_project.rds')
eis_net %v% 'ln(Readability score)' <- (log(readab$re[match(eis_net %v% 'vertex.names',readab$EIS.Number)]))
eis_net %v% 'Output' <- projects$DECISION[match(eis_net %v% 'vertex.names',projects$EIS.Number)]
eis_net %v% 'Year' <- as.character(floor(2013 + eis_net %v% 'Year'))
saveRDS(eis_net,'boilerplate_project/data_products/network_object.rds')
eis_consult_matrix = project_to_project_consult_matrix[rownames(project_to_project_consult_matrix) %in%  eis_ids,colnames(project_to_project_consult_matrix) %in%  eis_ids]
eis_consult_index = match(eis_net %v% 'vertex.names',rownames(eis_consult_matrix))
eis_consult_matrix = eis_consult_matrix[eis_consult_index,eis_consult_index]
eis_author_matrix = project_to_project_author_matrix[rownames(project_to_project_author_matrix) %in% eis_ids,colnames(project_to_project_author_matrix) %in% eis_ids]
eis_author_index = match(eis_net %v% 'vertex.names',rownames(eis_author_matrix))
eis_author_matrix = eis_author_matrix[eis_author_index,eis_author_index]
saveRDS(list('author' = eis_author_matrix,'consult' = eis_consult_matrix,'distance' = eis_distance_matrix),'boilerplate_project/data_products/edgecov_list.rds')
eis_net <- readRDS('boilerplate_project/data_products/network_object.rds')
mat_list = readRDS('boilerplate_project/data_products/edgecov_list.rds')
eis_author_matrix <- mat_list$author
eis_consult_matrix <- mat_list$consult
eis_distance_matrix <- mat_list$distance
eis_order = rank(eis_net %v% 'EIS')
temp_eis_net <- eis_net
list.vertex.attributes(temp_eis_net)
vertex_dt <- data.table(do.call(rbind,(lapply(list(
exp(temp_eis_net %v% 'ln_Pages_Hashed'),
exp(temp_eis_net %v% 'ln(Readability score)'),
temp_eis_net %v% 'NEPA_litigation_per_FEIS_2001_2012',
temp_eis_net %v% 'Skill_Rating',
temp_eis_net %v% 'Ideo_Rating'),summary,digits = 3)))[,c(1,3,4,6)])
rownames(vertex_dt) <- c('Pages','Readability','Litigations/FEIS',
'Skill rating','Ideo. rating')
tableout <-htmlTable(vertex_dt)
outdir.tables = "boilerplate_project/output/tables/"
sink(paste0(outdir.tables,"Table5part2.html"))
print(tableout,type="html",useViewer=F)
sink()
graph_dt <- data.table('attribute' = c('# projects','# ties','density'),
'value' = round(c(network.size(temp_eis_net),network.edgecount(temp_eis_net),network.density(temp_eis_net)),3))
tableout <-htmlTable(graph_dt)
outdir.tables = "boilerplate_project/output/tables/"
sink(paste0(outdir.tables,"Table5part1.html"))
print(tableout,type="html",useViewer=F)
sink()
edge_att_dt =data.table(round(do.call(rbind,list(
summary(eis_distance_matrix[upper.tri(eis_distance_matrix)]),
# summary(eis_author_matrix[upper.tri(eis_author_matrix)]),
#summary(eis_consult_matrix[upper.tri(eis_consult_matrix)]),
summary(c(abs(outer(temp_eis_net%v%"Fed_Reg_Date",temp_eis_net%v%"Fed_Reg_Date","-"))))))[,c(1,3,4,6)],3))
rownames(edge_att_dt) <- c('Euclidean distance','Publication date')
tableout <-htmlTable(edge_att_dt)
outdir.tables = "boilerplate_project/output/tables/"
sink(paste0(outdir.tables,"Table5part3.html"))
print(tableout,type="html",useViewer=F)
sink()
cons = table(temp_eis_net %v% 'Consultant')
outp = table(temp_eis_net %v% 'Output')
yrs = table(temp_eis_net %v% 'Year')
dt = data.table(c(
paste(paste0(names(outp),' = ',as.numeric(outp)),collapse = ' '),
paste(paste0(names(cons),' = ',as.numeric(cons)),collapse = ' '),
paste(paste0(names(yrs),' = ',as.numeric(yrs)),collapse = ' ')
))
rownames(dt) <- c('output type','used consultant?','year published')
tableout <-htmlTable(dt)
outdir.tables = "boilerplate_project/output/tables/"
sink(paste0(outdir.tables,"Table5part4.html"))
print(tableout,type="html",useViewer=F)
sink()
shar = htmlTable(rbind(
cbind(paste0(round(mean(eis_consult_matrix[upper.tri(eis_consult_matrix)])*100,2),'% of ij pairings'),
paste0(round(sum(rowSums(eis_consult_matrix)>0) / nrow(eis_consult_matrix),2) * 100,"% have a shared consultant")),
cbind(paste0(round(mean(eis_author_matrix[upper.tri(eis_author_matrix)])*100,2),'% of ij pairings'),
paste0(round(sum(rowSums(eis_author_matrix)>0) / nrow(eis_author_matrix),2) * 100,"% have a shared author"))
))
tableout <-shar
outdir.tables = "boilerplate_project/output/tables/"
sink(paste0(outdir.tables,"Table5part5.html"))
print(tableout,type="html",useViewer=F)
sink()
cores = detectCores()-2
cl = makeCluster(cores)
registerDoParallel(cl)
seed = 24
mod_lolog_cov = lolog(temp_eis_net ~ edges + #esp(0) +
gwdegree(1.5) +
gwesp(0.4) +
nodeFactor('Year') +
nodeCov('ln_Pages_Hashed') +
nodeMatch('Agency_Short') +
nodeFactor('Consultant')+
nodeCov('Skill_Rating') +
nodeCov('Ideo_Rating') +
nodeCov('NEPA_litigation_per_FEIS_2001_2012') +
absDiff('Fed_Reg_Date') +
nodeFactor('Output')+
nodeCov('ln(Readability score)') +
edgeCov(eis_author_matrix,'Shared_Author') +
edgeCov(eis_consult_matrix,'Shared_Consultant') +
edgeCov(eis_distance_matrix,'Centroid_Distance')|eis_order,
maxIter = 60,cluster = cl)
summary(mod_lolog_cv)
summary(mod_lolog_cov)
saveRDS(object = mod_lolog_cov,'boilerplate_project/data_products/lolog_withcovariates.rds')
saveRDS(object = summary(mod_lolog_cov),'boilerplate_project/data_products/lolog_summary_withcovariates.rds')
gof_deg_object = gofit(object = mod_lolog_cov,formula = temp_eis_net ~ degree(0:50),nsim = 1e3,cluster = cl)
gof_esp_object = gofit.lolog(object = mod_lolog_cov,formula = temp_eis_net ~ esp(0:25),nsim = 1e3,cluster = cl)
saveRDS(object = list(gof_deg_object,gof_esp_object),'boilerplate_project/data_products/lolog_gof.rds')
mod_lolog_cov <- readRDS('boilerplate_project/data_products/lolog_withcovariates.rds')
gof = readRDS('boilerplate_project/data_products/lolog_gof.rds')
gof = list(gof_deg_object,gof_esp_object)
dof = reshape2::melt(gof[[1]]$stats)
obs = reshape2::melt( gof[[1]]$ostats)
obs$degree = as.numeric(str_extract(rownames(obs),'[0-9]{1,}'))
g1 = ggplot() + geom_path(data = dof,aes(x = Var2-1,group = Var1,y = value),alpha = 0.2,lwd = 0.1) +
theme_bw() + geom_path(data = obs,aes(x = degree,y= value),lwd = 1,col = 'red') + xlab('# degrees') +
ylab('# of structures observed or simulated')
dof2 = reshape2::melt(gof[[2]]$stats)
obs2 = reshape2::melt( gof[[2]]$ostats)
obs2$degree = as.numeric(str_extract(rownames(obs2),'[0-9]{1,}'))
g2 = ggplot() + geom_path(data = dof2,aes(x = Var2-1,group = Var1,y = value,col ='black'),alpha = 0.3,lwd = 0.1) +
theme_bw() + geom_path(data = obs2,aes(x = degree,y= value,col = 'red'),lwd = 1) +
xlab('# edgewise shared partners') + ylab('# of structures observed or simulated') +
scale_color_manual(values= c('black','red'),labels = c('simulated graph','observed graph')) +
# guides(color = guide_legend(override.aes = list(lwd = c(1,1),labels= c('A','B'),col = c('1','2'))))
theme(legend.position = c(0.7,0.9),legend.title = element_blank(),legend.background = element_rect(fill = alpha('white',0)))
ggsave(grid.arrange(g1,g2,ncol = 2,top = 'Model goodness-of-fit: observed vs. simulated structures'),filename = 'boilerplate_project/output/figures/FigureA4_appendix_gof_plots.tiff',dpi = 500, width = 7,height = 3.5, units = 'in')
msum = readRDS('boilerplate_project/data_products/lolog_summary_withcovariates.rds')
m1 = (msum[c('theta','se','pvalue')]) %>% data.frame() %>% mutate(coef = rownames(.))
eis_mresults = m1
eis_mresults$DV = 'EIS'
eis_mresults$coef = fct_recode(eis_mresults$coef,'litigations per FEIS, 2001-2012' = 'nodecov.NEPA_litigation_per_FEIS_2001_2012','Used consultants' = 'nodeFactor.Consultant.1',
'|publication date diff.|' = 'absDiff.Fed_Reg_Date','ln(pages)' = 'nodecov.ln_Pages_Hashed','Same consultant(s)' = 'edgeCov.Shared_Consultant',
"Same preparer(s)" = "edgeCov.Shared_Author","Distance" = "edgeCov.Centroid_Distance",'ln(readability)' = "nodecov.ln(Readability score)",
'Same agency'='nodematch.Agency_Short', "Program/Policy" = "nodeFactor.Output.1", "Project" = "nodeFactor.Output.2",
'Ideol. rating' = 'nodecov.Ideo_Rating','Skill rating' = 'nodecov.Skill_Rating','GWESP (decay=2)'='gwesp.2','GW degree (decay=1)'='gwdegree.1',
'Program/policy' = 'nodeFactor.Output.1', 'Project' = 'nodeFactor.Output.2',
'year 2014' = 'nodeFactor.Year.1','year 2015'='nodeFactor.Year.2','year 2016'= 'nodeFactor.Year.3',
'year 2017' = 'nodeFactor.Year.4','year 2018'='nodeFactor.Year.5','year 2019'='nodeFactor.Year.6','year 2020'='nodeFactor.Year.7')
eis_mresults$coef = fct_inorder(eis_mresults$coef)
eis_mresults$coef = fct_rev(eis_mresults$coef)
eis_mresults <- data.table(eis_mresults)
eis_mresults[,sig:=ifelse(theta - 1.96 * se <0 & theta + 1.96 * se>0,0,1)]
eis_mresults$mod_sig = paste0(eis_mresults$mod,eis_mresults$sig)
eis_mresults[,CI:=paste0(sprintf("%.3f", round(theta,3)),' (',sprintf("%.3f", round(se,3)),')')]
eis_mresultscoef = fct_inorder(eis_mresults$coef)
eis_mresults$stars = ifelse(eis_mresults$pvalue <0.001,"***",ifelse(eis_mresults$pvalue<0.01,"**",ifelse(eis_mresults$pvalue<0.05,"*",'')))
eis_mresults$CI  =  paste0(eis_mresults$CI,eis_mresults$stars)
coef_cast =  eis_mresults[,.(coef,CI)]
coef_cast = coef_cast[order(fct_rev(coef)),]
outdir.tables = "boilerplate_project/output/tables/"
tableout <-htmlTable(coef_cast)
outdir.tables = "boilerplate_project/output/tables/"
sink(paste0(outdir.tables,"Table6_eis_lolog_coefs.html"))
print(tableout,type="html",useViewer=F)
sink()
pack = c("tidyverse","Matrix","statnet","data.table","stringr","lolog","texreg","textreuse","gridExtra","sf","lubridate","doParallel","htmlTable",'treemapify',"ggridges","ggplot2","viridis","hrbrthemes","forcats","betareg",'ggthemes','tm','pbapply')
need = !pack %in% installed.packages()[,"Package"]
#sudo apt-get install libfontconfig1-dev
if(any(need)){sapply(pack[need],install.packages)}
sapply(pack,require,character.only=T)
mcores = detectCores() - 2
samples = 10000
iters = 40
stepsize = 500
require(forcats)
projects = fread('boilerplate_project/data_products/project_candidates_eis_only.csv')
projects$EIS.Number <- as.character(projects$EIS.Number)
docs = fread('boilerplate_project/data_products/document_candidates_eis_only.csv')
flist_dt = readRDS('boilerplate_project/input/feis_corpus_2013-2020.rds')
flist_dt = flist_dt[str_replace(flist_dt$File,'txt$','pdf') %in% docs$FILE_NAME,]
source('boilerplate_project/code/functions/cleanText.R')
flist_dt <- cleanText(flist_dt)
flist_dt$EIS.Number <- str_extract(flist_dt$File,'^[0-9]{8}')
meta_dt = flist_dt[,.(Page,File,EIS.Number)]
page_counts = meta_dt[,.N,by=.(EIS.Number)]
setnames(page_counts,'N','total_pages')
projects[,.N,by=.(AGENCY)][order(-N)]
projects[,USE_DOCS:=EIS.Number %in% meta_dt$EIS.Number]
lda = readRDS('boilerplate_project/data_products/score_results/eis_page_scores_scratch_file.rds')
lda$score = as.numeric(lda$score)
lda = lda[!duplicated(lda),]
lda = lda[score>=300,]
lda$a = gsub('^(((?!--).)+-{2})\\1','\\1',lda$a,perl = T)
lda$b = gsub('^(((?!--).)+-{2})\\1','\\1',lda$b,perl = T)
lda = lda[a!=b,]
lda = lda[!duplicated(lda),]
gc()
lda$a_page = as.numeric(str_remove(lda$a,'.*_'))
lda$b_page = as.numeric(str_remove(lda$b,'.*_'))
lda$a_id = (str_remove(lda$a,'_.*'))
lda$b_id = (str_remove(lda$b,'_.*'))
lda$a_file = (str_remove(lda$a,'_[0-9]{1,}$'))
lda$b_file = (str_remove(lda$b,'_[0-9]{1,}$'))
lda <- lda[a_file %in% str_replace(docs$FILE_NAME,'pdf$','txt'),]
lda <- lda[b_file %in% str_replace(docs$FILE_NAME,'pdf$','txt'),]
lda_eis = lda[a_id == b_id,]
lda_solo = rbind(lda_eis[,.(a_id,score,a,a_file,a_page)],lda_eis[,.(b_id,score,b,b_file,b_page)],use.names = F)
lda_solo$AGENCY = projects$AGENCY[match(lda_solo$a_id,projects$EIS.Number)]
#lda_solo = lda_solo[!a_file %in% badfiles,]
lda_solo$a_file = gsub('\\s','_',lda_solo$a_file)
lda_solo$a_file = gsub('\\.pdf\\.txt$','.txt',lda_solo$a_file)
lda_solo$a = gsub('\\.pdf\\.txt','.txt',lda_solo$a)
over300 = lda_solo[order(-score),][!duplicated(a),]
countover300 = over300[,.N,by = .(a_id,AGENCY)]
setnames(countover300,'a_id','EIS.Number')
setnames(countover300,'N','over300')
page_counts = page_counts[page_counts$EIS.Number %in% projects$EIS.Number,]
page_counts$AGENCY = projects$AGENCY[match(page_counts$EIS.Number,projects$EIS.Number)]
countover300 = merge(countover300,page_counts[EIS.Number %in% projects$EIS.Number,],all = T)
countover300$AGENCY_SHORT = projects$AGENCY_SHORT[match(countover300$AGENCY,projects$AGENCY)]
countover300$over300[is.na(countover300$over300)] <- 0
agency_summary_stats = countover300[,list(mean(over300/total_pages),median(over300/total_pages)),by = .(AGENCY_SHORT)]
names(agency_summary_stats) <- c('AGENCY_SHORT','mean','median')
agency_summary_stats$mean = round(agency_summary_stats$mean,3)
agency_summary_stats$median = round(agency_summary_stats$median,3)
countover300$AGENCY_SHORT <- fct_infreq(countover300$AGENCY_SHORT)
(gg_pages_over_300 = ggplot() +
#  geom_jitter(data = countover300,aes(x = fct_rev(AGENCY_SHORT),y = over300/total_pages),pch = 21,colour = 'grey50') +
#geom_boxplot(data = countover300,aes(x = fct_rev(AGENCY_SHORT),y = over300/total_pages),fill = NA) +
geom_jitter(data = countover300,aes(x = fct_rev(AGENCY_SHORT),y = over300/total_pages),alpha = 0.5,fill = NA,height = 0,pch = 21) +
geom_point(data = countover300[,median(over300/total_pages),by=.(AGENCY_SHORT)],
aes(x = fct_rev(AGENCY_SHORT),y = V1,col ='red'),pch = '|',size = 5)+
coord_flip() + scale_color_manual(values = 'red',name = 'Agency median',labels = 'Agency median')+
#    ggplot2::annotate("label",x = fct_rev(agency_summary_stats$AGENCY_SHORT),y = agency_summary_stats$median*1.5,label = agency_summary_stats$median) +
theme_bw() + scale_y_continuous(name = '# pages w/ LDA>300 / total pages by project') +
#limits=c(0,round(max(countover300$over300/countover300$total_pages) ,1))) +
theme(legend.title = element_blank(),axis.title.y = element_blank(),legend.position = c(0.8,0.25),axis.text = element_text(size = 12)) +
ggtitle("Text reuse within EISs by project and agency"))
ggsave(gg_pages_over_300,filename = 'boilerplate_project/output/figures/Figure5_agencies_within_lda_over_300.tiff',dpi = 300,units = 'in',height = 4.5,width = 6)
eis_ids = unique(projects$EIS.Number[projects$USE_DOCS])
n_eis = length(eis_ids)
projects = projects[USE_DOCS==T&!duplicated(EIS.Number),]
projects = merge(projects,countover300[,.(EIS.Number,total_pages,over300)],by='EIS.Number')
readab = readRDS('boilerplate_project/data_products/readability_scores_by_project.rds')
projects$`Readability score` <- (log(readab$re[match(projects$EIS.Number,readab$EIS.Number)]))
lit_dt = fread('boilerplate_project/data_products/nepa_litigation_by_agency_year.csv')
ldt_long = melt(lit_dt,id.vars = 'recode',measure.vars = patterns('y[0-9]'))
ldt_long$year <- as.numeric(str_remove(ldt_long$variable,'[a-z]'))
ltd_total = ldt_long[year<2013,][,sum(value,na.rm = T),by=.(recode)]
tots = projects[,.N,by=.(AGENCY_SHORT)]
ltd_total$total_eis = tots$N[match(ltd_total$recode,tots$AGENCY_SHORT)]
ltd_total$litigation_per_eis = ltd_total$V1/ltd_total$total_eis
projects$litigation_per_eis <- ltd_total$litigation_per_eis[match(projects$AGENCY_SHORT,ltd_total$recode)]
ideology_sheet = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSOSX--qpOSyBZcW3OWeWRmiC2-eC74un7fZYXFCzrW8FNB1FOQFMaIq-CW8hMIoBqtZMXYR05UA7Lu/pub?output=csv'
ideol = fread(ideology_sheet)
subs = str_split(ideol$epaStandIn,', ')
projects$index = match(projects$Lead.Agency,ideol$epaName)
projects = data.table(projects,ideol[unlist(sapply(projects$Lead.Agency,function(x) c(grep(x,ideol$epaName),grep(x,ideol$epaStandIn)))),])
projects$skills_rating[is.na(projects$skills_rating)] <- 0
consults = readRDS('boilerplate_project/data_products/consultant_project_matches.RDS')
setnames(consults,'PROJECT_ID','EIS.Number')
projects$USED_CONSULTANT <- (projects$EIS.Number %in% consults$EIS.Number) + 0
projects$beta_dv <- {{projects$over300/projects$total_pages} * (nrow(projects)-1) + mean(projects$over300/projects$total_pages)} / nrow(projects)
beta_mod = betareg(beta_dv~log(total_pages) + log(`Readability score`) + litigation_per_eis +
ideo_rating + skills_rating + DECISION + as.factor(Year) + USED_CONSULTANT,data = projects)
summary(lm(over300/total_pages~log(total_pages) + log(`Readability score`) + litigation_per_eis +
ideo_rating + skills_rating + DECISION + as.factor(Year) + USED_CONSULTANT,data = projects))
htmlreg(beta_mod,file = 'boilerplate_project/output/tables/Table7_betaregression_results.html',single.row = T)
pack = c("tidyverse","Matrix","statnet","data.table","stringr","lolog","textreuse","gridExtra","sf","lubridate","doParallel","htmlTable",'treemapify',"ggridges","ggplot2","viridis","hrbrthemes","forcats",'ggthemes','tm','pbapply')
need = !pack %in% installed.packages()[,"Package"]
#sudo apt-get install libfontconfig1-dev
if(any(need)){sapply(pack[need],install.packages)}
sapply(pack,require,character.only=T)
projects = fread('boilerplate_project/data_products/project_candidates_eis_only.csv')
docs = fread('boilerplate_project/data_products/document_candidates_eis_only.csv')
flist_dt = readRDS('boilerplate_project/input/feis_corpus_2013-2020.rds')
flist_dt = flist_dt[str_replace(flist_dt$File,'txt$','pdf') %in% docs$FILE_NAME,]
source('boilerplate_project/code/functions/cleanText.R')
flist_dt <- cleanText(flist_dt)
flist_dt$EIS.Number <- str_extract(flist_dt$File,'^[0-9]{8}')
meta_dt = flist_dt[,.(Page,File,EIS.Number)]
page_counts = meta_dt[,.N,by=.(EIS.Number)]
setnames(page_counts,'N','total_pages')
projects[,.N,by=.(AGENCY)][order(-N)]
projects[,USE_DOCS:=EIS.Number %in% meta_dt$EIS.Number]
lda = readRDS('boilerplate_project/data_products/score_results/eis_page_scores_scratch_file.rds')
lda$score = as.numeric(lda$score)
lda = lda[!duplicated(lda),]
lda = lda[score>=300,]
lda$a = gsub('^(((?!--).)+-{2})\\1','\\1',lda$a,perl = T)
lda$b = gsub('^(((?!--).)+-{2})\\1','\\1',lda$b,perl = T)
lda = lda[a!=b,]
lda = lda[!duplicated(lda),]
gc()
lda$a_page = as.numeric(str_remove(lda$a,'.*_'))
lda$b_page = as.numeric(str_remove(lda$b,'.*_'))
lda$a_id = (str_remove(lda$a,'_.*'))
lda$b_id = (str_remove(lda$b,'_.*'))
lda$a_file = (str_remove(lda$a,'_[0-9]{1,}$'))
lda$b_file = (str_remove(lda$b,'_[0-9]{1,}$'))
lda <- lda[a_file %in% str_replace(docs$FILE_NAME,'pdf$','txt'),]
lda <- lda[b_file %in% str_replace(docs$FILE_NAME,'pdf$','txt'),]
lda_within = lda[a_id == b_id,]
lda_solo = rbind(lda_within[,.(a_id,score,a,a_file,a_page)],lda_within[,.(b_id,score,b,b_file,b_page)],use.names = F)
lda_solo$AGENCY = projects$AGENCY[match(lda_solo$a_id,projects$EIS.Number)]
#lda_solo = lda_solo[!a_file %in% badfiles,]
lda_solo$a_file = gsub('\\s','_',lda_solo$a_file)
lda_solo$a_file = gsub('\\.pdf\\.txt$','.txt',lda_solo$a_file)
lda_solo$a = gsub('\\.pdf\\.txt','.txt',lda_solo$a)
over300 = lda_solo[order(-score),][!duplicated(a),]
countover300 = over300[,.N,by = .(a_id,AGENCY)]
setnames(countover300,'a_id','EIS.Number')
setnames(countover300,'N','over300')
page_counts = page_counts[page_counts$EIS.Number %in% projects$EIS.Number,]
page_counts$AGENCY = projects$AGENCY[match(page_counts$EIS.Number,projects$EIS.Number)]
countover300 = merge(countover300,page_counts[EIS.Number %in% projects$EIS.Number,],all = T)
countover300$AGENCY_SHORT = projects$AGENCY_SHORT[match(countover300$AGENCY,projects$AGENCY)]
countover300$over300[is.na(countover300$over300)] <- 0
projects$within_prop = {countover300$over300/countover300$total_pages}[match(projects$EIS.Number,countover300$EIS.Number)]
lda_between = lda[a_id != b_id,]
lda_solo = rbind(lda_between[,.(a_id,score,a,a_file,a_page)],lda_between[,.(b_id,score,b,b_file,b_page)],use.names = F)
lda_solo$AGENCY = projects$AGENCY[match(lda_solo$a_id,projects$EIS.Number)]
#lda_solo = lda_solo[!a_file %in% badfiles,]
lda_solo$a_file = gsub('\\s','_',lda_solo$a_file)
lda_solo$a_file = gsub('\\.pdf\\.txt$','.txt',lda_solo$a_file)
lda_solo$a = gsub('\\.pdf\\.txt','.txt',lda_solo$a)
over300 = lda_solo[order(-score),][!duplicated(a),]
countover300 = over300[,.N,by = .(a_id,AGENCY)]
setnames(countover300,'a_id','EIS.Number')
setnames(countover300,'N','over300')
page_counts = page_counts[page_counts$EIS.Number %in% projects$EIS.Number,]
page_counts$AGENCY = projects$AGENCY[match(page_counts$EIS.Number,projects$EIS.Number)]
countover300 = merge(countover300,page_counts[EIS.Number %in% projects$EIS.Number,],all = T)
countover300$AGENCY_SHORT = projects$AGENCY_SHORT[match(countover300$AGENCY,projects$AGENCY)]
countover300$over300[is.na(countover300$over300)] <- 0
projects$between_prop = {countover300$over300/countover300$total_pages}[match(projects$EIS.Number,countover300$EIS.Number)]
projects = projects[USE_DOCS==T&!duplicated(EIS.Number),]
projects$total_pages = page_counts$total_pages[match(projects$EIS.Number,page_counts$EIS.Number)]
(compare_between_within = ggplot(projects,aes(x = between_prop,y = within_prop,size = total_pages)) +
geom_point(alpha = 0.5,pch = 21) +
scale_x_continuous('Proportion of pages w/ between-EIS LDA score > 300') +
scale_y_continuous('Proportion of pages w/ within-EIS LDA score > 300') +
theme_bw() +
theme(legend.position = c(0.8,0.7),legend.background = element_rect(fill = alpha('white',0))) +
scale_size_binned_area(breaks=c(100,500,1000,10000),name = '# EIS pages') +
ggtitle('Between-project vs. within-project text reuse'))
ggsave(compare_between_within,filename = 'boilerplate_project/output/figures/Figure6_compare_within_between.tiff',dpi = 400,width = 7,height = 6,units = 'in')
consult_Freq
consult_dt
consult
consults
consults[grepl('EARTH SYSTEMS',FIRM)]
test = readRDS('boilerplate_project/input/detected_preparer_pages_uncleaned.rds')
test[EIS.Number=='20180255']
test[grepl('20180255',FILE),]
test[grepl('20180255',FILE),]$Page
consults[EIS.Number=='20180255']
consults[FIRM == 'LEIDOS']
consults = readRDS('boilerplate_project/data_products/consultant_project_matches.RDS')
setnames(consults,'PROJECT_ID','EIS.Number')
consult_Freq = consults[EIS.Number %in% projects$EIS.Number[projects$USE_DOCS],][,.N,by = .(FIRM)][order(-N)][N>=10,]
consult_Freq
consult_Freq = consults[EIS.Number %in% projects$EIS.Number[projects$USE_DOCS],][,.N,by = .(FIRM)][order(-N)][N>10,]
consult_Freq
consult_Freq[FIRM!='EARTH SYSTEMS INC.']
consults = readRDS('boilerplate_project/data_products/consultant_project_matches.RDS')
setnames(consults,'PROJECT_ID','EIS.Number')
consult_Freq = consults[EIS.Number %in% projects$EIS.Number[projects$USE_DOCS],][,.N,by = .(FIRM)][order(-N)][N>10,]
#false positive
consult_Freq = consult_Freq[FIRM!='EARTH SYSTEMS INC.']
tableout <-htmlTable(consult_Freq)
outdir.tables = 'boilerplate_project/output/tables/'
sink(paste0(outdir.tables,"TableA1_consultant_table.html"))
print(tableout,type="html",useViewer=F)
sink()
grep('arcadis',tolower(test$text))
test[grepl('20180255',FILE),]$FILE
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','ggrepel','DescTools','lubridate')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require,character.only = T)
redraw_corpus = TRUE
projs = readRDS('climate_in_eis_project/data_products/deis_metadata_with_covariates.RDS')
docs = readRDS('climate_in_eis_project/data_products/deis_doc_metadata.RDS')
risks = readRDS('climate_in_eis_project/data_products/project_risks.RDS')
setnames(risks, 'PROJECT_ID','EIS.Number')
projs$EIS.Number <- as.character(projs$EIS.Number)
projs <- merge(projs,risks,all.x = T)
quanteda_toks = list.files('climate_in_eis_project/yearly_quanteda_tokens/',full.names = T)
tok_list = lapply(quanteda_toks,readRDS)
#do.call is good code, but takes _forever_
#tok_all =do.call("+",tok_list)
#this is bad code, but goes fast
tok_all = tok_list[[1]]
for(t in tok_list[-1]){
tok_all = tok_all + t
}
qdfm = dfm(tok_all)
dim(qdfm)
qdfm <- qdfm[,qdfm@Dimnames$features %in% eis3]
eis3 <- qdfm_eis@Dimnames$features
qdfm_eis <- dfm_group(qdfm,str_extract(qdfm_freq@Dimnames$docs,'^[0-9]{8}'))
qdfm_eis <- dfm_group(qdfm,str_extract(qdfm@Dimnames$docs,'^[0-9]{8}'))
qdfm_eis <- dfm_trim(qdfm_eis,min_docfreq = 3)
eis3 <- qdfm_eis@Dimnames$features
dim(qdfm)
qdfm <- qdfm[,qdfm@Dimnames$features %in% eis3]
dim(qdfm)
qdfm_freq <- dfm_trim(qdfm_freq, max_docfreq = 0.3,docfreq_type = 'prop')
qdfm <- dfm_trim(qdfm, max_docfreq = 0.3,docfreq_type = 'prop')
dim(qdfm)
meta_eis = projs[match(str_remove(qdfm_freq@Dimnames$docs,'_.*'),projs$EIS.Number),]
meta_eis$EIS.Number <- as.character(meta_eis$EIS.Number)
meta_eis$ID = qdfm_freq@Dimnames$docs
meta_eis = projs[match(str_remove(qdfm@Dimnames$docs,'_.*'),projs$EIS.Number),]
meta_eis$EIS.Number <- as.character(meta_eis$EIS.Number)
meta_eis$ID = qdfm@Dimnames$docs
K = 90
dfm2stm <- convert(qdfm, to = "stm")
meta_eis_sub = meta_eis[ID %in% names(dfm2stm$documents),]
meta_eis_sub$YEAR = str_extract(meta_eis_sub$EIS.Number,'^[0-9]{4}')
dfm2stm$meta = meta_eis_sub
dfm2stm$meta$project_EAL[is.na(dfm2stm$meta$project_EAL)] <- median(dfm2stm$meta$project_EAL,na.rm = T)
dfm2stm$meta$project_CR[is.na(dfm2stm$meta$project_CR)] <- median(dfm2stm$meta$project_CR,na.rm = T)
dfm2stm$meta$project_SVI[is.na(dfm2stm$meta$project_SVI)] <- median(dfm2stm$meta$project_SVI,na.rm = T)
#use k = 0 to automate guess to understand range of k
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = K, data = dfm2stm$meta,
prevalence = ~EIS.Number + AGENCY + YEAR + PROJECT_TYPE + PROJECT_TOPIC+
s(project_EAL) + s(project_SVI) + s(project_CR),
init.type = "Spectral",verbose = T,ngroups = 5,
seed = 24,max.em.its = 40,emtol = 0.0001)
saveRDS(model.stm,'climate_in_eis_project/scratch/temp_stm_90k.RDS')
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','ggrepel','DescTools','lubridate')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require,character.only = T)
install.packages('DescTools',type = 'source')
'climate_in_eis_project/scratch/
install.packges('DescTools')
install.packages('DescTools')
install.packages('stm')
install.packages("glmnet")
install.packages("glmnet",type = 'source')
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','ggrepel','DescTools','lubridate')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require,character.only = T)
redraw_corpus = TRUE
projs = readRDS('climate_in_eis_project/data_products/deis_metadata_with_covariates.RDS')
docs = readRDS('climate_in_eis_project/data_products/deis_doc_metadata.RDS')
risks = readRDS('climate_in_eis_project/data_products/project_risks.RDS')
setnames(risks, 'PROJECT_ID','EIS.Number')
projs$EIS.Number <- as.character(projs$EIS.Number)
projs <- merge(projs,risks,all.x = T)
quanteda_toks = list.files('climate_in_eis_project/yearly_quanteda_tokens/',full.names = T)
tok_list = lapply(quanteda_toks,readRDS)
#do.call is good code, but takes _forever_
#tok_all =do.call("+",tok_list)
#this is bad code, but goes fast
tok_all = tok_list[[1]]
for(t in tok_list[-1]){
tok_all = tok_all + t
}
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','ggrepel','DescTools','lubridate')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require,character.only = T)
redraw_corpus = TRUE
projs = readRDS('climate_in_eis_project/data_products/deis_metadata_with_covariates.RDS')
docs = readRDS('climate_in_eis_project/data_products/deis_doc_metadata.RDS')
risks = readRDS('climate_in_eis_project/data_products/project_risks.RDS')
setnames(risks, 'PROJECT_ID','EIS.Number')
projs$EIS.Number <- as.character(projs$EIS.Number)
projs <- merge(projs,risks,all.x = T)
quanteda_toks = list.files('climate_in_eis_project/yearly_quanteda_tokens/',full.names = T)
tok_list = lapply(quanteda_toks,readRDS)
#do.call is good code, but takes _forever_
#tok_all =do.call("+",tok_list)
#this is bad code, but goes fast
tok_all = tok_list[[1]]
for(t in tok_list[-1]){
tok_all = tok_all + t
}
qdfm = dfm(tok_all)
qdfm_eis <- dfm_group(qdfm,str_extract(qdfm@Dimnames$docs,'^[0-9]{8}'))
qdfm_eis <- dfm_trim(qdfm_eis,min_docfreq = 3)
eis3 <- qdfm_eis@Dimnames$features
qdfm <- qdfm[,qdfm@Dimnames$features %in% eis3]
qdfm <- dfm_trim(qdfm, max_docfreq = 0.3,docfreq_type = 'prop')
qdfm_stem = dfm_wordstem(qdfm)
qdfm_stem <- qdfm_stem[ntoken(qdfm_stem)>0,]
meta_eis = projs[match(str_remove(qdfm_stem@Dimnames$docs,'_.*'),projs$EIS.Number),]
meta_eis$EIS.Number <- as.character(meta_eis$EIS.Number)
meta_eis$ID = qdfm_stem@Dimnames$docs
dfm2stm <- convert(qdfm_stem, to = "stm")
meta_eis_sub = meta_eis[ID %in% names(dfm2stm$documents),]
meta_eis_sub$YEAR = str_extract(meta_eis_sub$EIS.Number,'^[0-9]{4}')
dfm2stm$meta = meta_eis_sub
dfm2stm$meta$project_EAL[is.na(dfm2stm$meta$project_EAL)] <- median(dfm2stm$meta$project_EAL,na.rm = T)
dfm2stm$meta$project_CR[is.na(dfm2stm$meta$project_CR)] <- median(dfm2stm$meta$project_CR,na.rm = T)
dfm2stm$meta$project_SVI[is.na(dfm2stm$meta$project_SVI)] <- median(dfm2stm$meta$project_SVI,na.rm = T)
K = 80
model.cov <- stm(dfm2stm$documents, dfm2stm$vocab, K = K, data = dfm2stm$meta,
prevalence = ~EIS.Number + AGENCY + YEAR + PROJECT_TYPE + PROJECT_TOPIC+
s(project_EAL) + s(project_SVI) + s(project_CR),
init.type = "Spectral",verbose = T,#ngroups = 5,
seed = 24,max.em.its = 40,emtol = 0.0001)
saveRDS(model.cov,'climate_in_eis_project/scratch/base_stm_80k.RDS')
2+2
