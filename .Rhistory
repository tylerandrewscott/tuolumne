coef11$topic = '11: climate-->project'
coef11$coef = rownames(coef_sum$tables[[1]])
coef28 = data.table(coef_sum$tables[[2]])
coef28$topic = '28: project-->climate'
coef28$coef = rownames(coef_sum$tables[[2]])
coefs = rbind(coef11,coef28)
coefs$GROUP[grepl('YEAR',coefs$coef)]<-'Year'
coefs$GROUP[grepl('AGENCY',coefs$coef)]<-'Agency'
coefs$GROUP[grepl('TOPIC',coefs$coef)]<-'Focus'
coefs$GROUP[grepl('TYPE',coefs$coef)]<-'Task'
coefs$GROUP[grepl('skill|ideo',coefs$coef)]<-'Skill/Ideol.'
coefs$coef <- gsub('ideo','Ideology',coefs$coef)
coefs$coef <- gsub('skill','Workforce skill',coefs$coef)
coefs$coef <- gsub('PROJECT_TOPIC','Topic: ',coefs$coef)
coefs$coef <- gsub('PROJECT_TYPE','Task: ',coefs$coef)
coefs$coef <- gsub('YEAR','Year: ',coefs$coef)
coefs$coef <- gsub('AGENCY','',coefs$coef)
require(forcats)
require(ggthemes)
coefs$coef <- fct_inorder(coefs$coef)
coefs$coef <- fct_rev(coefs$coef)
topic_task_coefs = coefs[!grepl('Agency|Year',GROUP)&!grepl('Intercept|rating',coef),]
topic_task_coefs$topic_or_task = ifelse(grepl('Topic',topic_task_coefs$coef),'Topic','Task')
ggplot(topic_task_coefs) + #facet_wrap(~topic)+
geom_vline(xintercept = 0,lty = 2,col = 'grey80')+
geom_errorbarh(aes(xmin = Estimate-1.96*`Std. Error`,xmax = Estimate+1.96*`Std. Error`,
y= coef,col = topic),height = 0.2,position = position_dodge(0.3))+
geom_point(aes(x = Estimate,y = coef,col = topic),position = position_dodge(0.3)) +
scale_color_colorblind()+ scale_x_continuous(limits=c(-0.02,0.02))+
ylab('') + theme_bw() +
# facet_wrap(~topic_or_task)+
theme(axis.title.y = element_blank(),legend.position = c(0.75,0.25),axis.text = element_text(size = 12)) +
xlab('Estimate 95% confidence intervals') + ggtitle('Topic focus regressed on focus and task')
coef_sum = summary(eff_all_un)
coef11 = data.table(coef_sum$tables[[1]])
coef11$topic = '11: climate-->project'
coef11$coef = rownames(coef_sum$tables[[1]])
coef28 = data.table(coef_sum$tables[[2]])
coef28$topic = '28: project-->climate'
coef28$coef = rownames(coef_sum$tables[[2]])
coefs = rbind(coef11,coef28)
coefs$GROUP[grepl('YEAR',coefs$coef)]<-'Year'
coefs$GROUP[grepl('AGENCY',coefs$coef)]<-'Agency'
coefs$GROUP[grepl('TOPIC',coefs$coef)]<-'Focus'
coefs$GROUP[grepl('TYPE',coefs$coef)]<-'Task'
coefs$GROUP[grepl('skill|ideo',coefs$coef)]<-'Skill/Ideol.'
coefs$coef <- gsub('ideo','Ideology',coefs$coef)
coefs$coef <- gsub('skill','Workforce skill',coefs$coef)
coefs$coef <- gsub('PROJECT_TOPIC','Focus: ',coefs$coef)
coefs$coef <- gsub('PROJECT_TYPE','Task: ',coefs$coef)
coefs$coef <- gsub('YEAR','Year: ',coefs$coef)
coefs$coef <- gsub('AGENCY','',coefs$coef)
require(forcats)
require(ggthemes)
coefs$coef <- fct_inorder(coefs$coef)
coefs$coef <- fct_rev(coefs$coef)
topic_task_coefs = coefs[!grepl('Agency|Year',GROUP)&!grepl('Intercept|rating',coef),]
topic_task_coefs$topic_or_task = ifelse(grepl('Topic',topic_task_coefs$coef),'Topic','Task')
ggplot(topic_task_coefs) + #facet_wrap(~topic)+
geom_vline(xintercept = 0,lty = 2,col = 'grey80')+
geom_errorbarh(aes(xmin = Estimate-1.96*`Std. Error`,xmax = Estimate+1.96*`Std. Error`,
y= coef,col = topic),height = 0.2,position = position_dodge(0.3))+
geom_point(aes(x = Estimate,y = coef,col = topic),position = position_dodge(0.3)) +
scale_color_colorblind()+ scale_x_continuous(limits=c(-0.02,0.02))+
ylab('') + theme_bw() +
# facet_wrap(~topic_or_task)+
theme(axis.title.y = element_blank(),legend.position = c(0.75,0.25),axis.text = element_text(size = 12)) +
xlab('Estimate 95% confidence intervals') + ggtitle('Topic focus regressed on focus and task')
pack = c('data.table','stringr','tidyverse','doParallel','pdftools','lubridate')
need = pack[!pack %in% installed.packages()[,'Package']]
lapply(need,install.packages)
lapply(pack,require,character.only=T)
empty_project_record = data.table(PROJECT_ID = character(),YEAR = numeric(),PROJECT_TYPE = character(),AGENCY = character())
empty_doc_dt = data.table(YEAR = numeric(),FILE_NAME = character(), FILE_LOC = character(), PROJECT_TYPE = character(),AGENCY = character())
###############
###############
epa = fread('../eis_documents/enepa_repository/meta_data/eis_record_detail.csv')
epa = epa[Document=='Final',]
epa = epa[grepl('^201[3-9]',epa$EIS.Number),]
#epa = epa[Agency=='Bureau of Land Management',]
epa$Title = iconv(epa$Title,'utf8')
epa$Year = str_extract(epa$EIS.Number,'^[0-9]{4}')
epa  = epa[!grepl('ADOPTION|WITHDRAWN|^Withdrawn|^Adoption',Title)&!grepl('PRO',State.or.Territory),]
epa[EIS.Number%in% c('20170008','20170006'),]
epa[grepl('Gateway|Transwest',Title),]
epa[grepl('Energy Gateway|Transwest',Title),]
epa[grepl('Energy Gateway|Trans(W|w)est',Title),]
# library(tm)
# library(quanteda)
# library(data.table)
# library(textclean)
# library(stringr)
# library(textmineR)
# library(stm)
# library(textmineR)
# library(textreuse)
# library(text2vec)
# library(pbapply)
#setwd('bucket_mount/tuolumne/')
rerun_existing = T
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(tm)){install.packages('tm');require(tm)}
if(!require(quanteda)){install.packages('quanteda');require(quanteda)}
if(!require(textclean)){install.packages('textclean');require(textclean)}
if(!require(stringr)){install.packages('stringr');require(stringr)}
if(!require(pbapply)){install.packages('pbapply');require(pbapply)}
if(!require(parallel)){install.packages('parallel');require(parallel)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
if(!require(textreuse)){install.packages('textreuse');require(textreuse)}
#setwd("google_drive/tuolumne/")
#setDTthreads(3)
#getDTthreads()
mcores = detectCores() / 2
options("mc.cores" = detectCores() /2)
#table(projects$AGENCY,projects$PROJECT_TYPE)
projects = fread('scratch/boilerplate/project_candidates_eis_only.csv')
projects = projects[Document=='Final',]
projects = projects[grepl('^201[3-9]|^2020',PROJECT_ID),]
documents = fread( 'scratch/boilerplate/document_candidates_eis_only.csv')
documents = documents[PROJECT_ID %in% projects$PROJECT_ID,]
scratch_loc = 'scratch/boilerplate/hash_candidates/'
tokpars = 'scratch/tokenized_paragraphs/'
flist  = list.files(tokpars)
flist <- flist[grepl('^201[3-9]|^2020',flist)]
flist <- flist[!grepl('^[0-9]{8}_(CEQ|\\1)',)]
flist <- flist[flist %in% gsub('pdf$|PDF$','txt',documents$File_Name)]
flist = flist[!grepl('^([0-9]{8})_(CEQ|\\1)',flist)]
minhash <- minhash_generator(n = 240, seed = 40)
progress_bars = T
gc()
cluster = makeCluster(mcores)
registerDoParallel(cl = cluster,cores = mcores)
parallel::clusterEvalQ(cluster,'require(data.table)')
#####
hash_file = paste0(scratch_loc,'eis_page_hash_paragraph_candidates.RDS')
tlist = foreach(i = seq_along(flist)) %dopar% {
t1 = readLines(paste0(tokpars,flist[i]))
t1}
names(tlist)<- flist
full_tlist = unlist(tlist)
rm(tlist)
gc()
saveRDS(full_flist,'scratch/paragraph_list.rds')
saveRDS(full_tlist,'scratch/paragraph_list.rds')
# library(tm)
# library(quanteda)
# library(data.table)
# library(textclean)
# library(stringr)
# library(textmineR)
# library(stm)
# library(textmineR)
# library(textreuse)
# library(text2vec)
# library(pbapply)
#setwd('bucket_mount/tuolumne/')
rerun_existing = T
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(tm)){install.packages('tm');require(tm)}
if(!require(quanteda)){install.packages('quanteda');require(quanteda)}
if(!require(textclean)){install.packages('textclean');require(textclean)}
if(!require(stringr)){install.packages('stringr');require(stringr)}
if(!require(pbapply)){install.packages('pbapply');require(pbapply)}
if(!require(parallel)){install.packages('parallel');require(parallel)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
if(!require(textreuse)){install.packages('textreuse');require(textreuse)}
if(!require(tidyverse)){install.packages('tidyverse');require(tidyverse)}
#setwd("google_drive/tuolumne/")
#setDTthreads(3)
#getDTthreads()
mcores = floor(detectCores() - 15 )
options("mc.cores" = floor(detectCores() - 15))
#table(projects$AGENCY,projects$PROJECT_TYPE)
projects = fread('../bucket_mount/tuolumne/scratch/boilerplate/project_candidates_eis_only.csv')
projects = projects[Document=='Final',]
projects = projects[grepl('^201[3-9]|^2020',PROJECT_ID),]
grep('PROGRAMMA',projects$Title,value = T)
scratch_loc = '../bucket_mount/tuolumne/scratch/boilerplate/hash_candidates/'
minhash <- minhash_generator(n = 240, seed = 40)
progress_bars = T
gc()
cluster = makeCluster(mcores)
registerDoParallel(cl = cluster,cores = mcores)
parallel::clusterEvalQ(cluster,'require(data.table)')
parallel::clusterEvalQ(cluster,'require(textreuse)')
#hash_file = paste0('../bucket_mount/big_eis_text.rds')
full_tlist <- readRDS('../bucket_mount/tuolumne/scratch/boilerplate/big_text_files/big_eis_text.rds')
chars = nchar(full_tlist$text)
periods = stringr::str_count(full_tlist$text,"\\.")
numbers = stringr::str_count(full_tlist$text,"[0-9]")
caps = stringr::str_count(full_tlist$text,'[A-Z]')
tildes = stringr::str_count(full_tlist$text,'~')
quotes = stringr::str_count(full_tlist$text,'\\"')
spaces = stringr::str_count(full_tlist$text,'\\s')
cut = 0.1
full_tlist  = full_tlist[chars>400&{periods/chars}<cut&{quotes/chars}<cut&{tildes/chars}<cut&{numbers/chars}<cut&{caps/chars}<cut&{spaces/chars}<{cut*2},]
#####
hash_file = paste0(scratch_loc,'eis_page_hashes.rds')
flist = as.character(full_tlist$text)
names(flist) <- paste0(full_tlist$File,'_',full_tlist$Page)
eis_corpus =  TextReuseCorpus(text = flist,
meta = list(File = full_tlist$File,Page = full_tlist$Page),
tokenizer = tokenize_ngrams, n = 10,
minhash_func = minhash, keep_tokens = TRUE,
progress = progress_bars,skip_short = T)
gc()
saveRDS(eis_corpus,'../bucket_mount/tuolumne/scratch/eis_page_corp_scratch.rds')
#eis_corpus = readRDS('../bucket_mount/tuolumne/scratch/eis_page_corp_scratch.rds')
#file.exists('../bucket_mount/tuolumne/scratch/eis_page_corp_scratch.rds')
split_corpus_ntiles = dplyr::ntile(x = seq(eis_corpus),n = mcores*10)
split_corpus = split(eis_corpus,split_corpus_ntiles)
split_buckets = foreach(x = split_corpus) %dopar% {textreuse::lsh(x,bands = 60)}
gc()
#
while(any(sapply(split_buckets,is.null))){
null_fails = which(sapply(split_buckets,is.null))
split_buckets[null_fails] <- pblapply(null_fails,function(x) lsh(split_corpus[[x]],bands = 40,progress = F),cl = 5)
}
eis_buckets = do.call(rbind,split_buckets)
saveRDS(eis_buckets,'../bucket_mount/tuolumne/scratch/eis_page_buckets_scratch.rds')
#eis_buckets = readRDS('../bucket_mount/tuolumne/scratch/eis_page_buckets_scratch.rds')
eis_candidates <- lsh_candidates(buckets = eis_buckets)
require(dplyr)
candidate_splits = split(eis_candidates,ntile(1:nrow(eis_candidates),n = nrow(eis_candidates) %/% 10000))
gc()
#corpus_meta = eis_corpus$meta
#corpus_meta$names <- names(eis_corpus)[!names(eis_corpus)%in%skipped(eis_corpus)]
require(doParallel)
score_list = foreach(i = candidate_splits) %dopar% {
send_names = unique(c(i$a,i$b))
send_text = flist[send_names]
score_list = mapply(function(aa,bb) textreuse::align_local(a = aa,b=bb)$score,aa = send_text[i$a],bb = send_text[i$b])
data.table::data.table(a = i$a, b= i$b,score = score_list)
}
score_dt = rbindlist(score_list)
saveRDS(score_dt,'../bucket_mount/tuolumne/scratch/eis_page_scores_scratch.rds')
if(!require(tokenizers)){install.packages('tokenizers');require(tokenizers)}
if(!require(spacyr)){install.packages('spacyr');require(spacyr)}
if(!require(tigris)){install.packages('tigris');require(tigris)}
if(!require(maptools)){install.packages('maptools');require(maptools)}
if(!require(acs)){install.packages('acs');require(acs)}
if(!require(pbapply)){install.packages('pbapply');require(pbapply)}
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(sf)){install.packages('sf');require(sf)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
#spacy_install(conda = "auto", version = "latest", lang_models = "en",
#              python_version = "3.6", envname = "spacy_condaenv", pip = FALSE,
#              python_path = NULL, prompt = TRUE)
#spacyr::spacy_download_langmodel(model = model,)
model = 'en_core_web_sm'
spacyr::spacy_initialize(model)
floc = 'scratch/'
#saveloc = 'temp_products/'
cors = detectCores() - 4
cl = makeCluster(cors)
registerDoParallel(cl)
full_tlist <- readRDS('../bucket_mount/tuolumne/scratch/boilerplate/big_text_files/big_eis_text.rds')
top40pages = full_tlist[Page %in% 1:50,]
county_tigris = tigris::counties(class = 'sf',year = '2017')
state_tigris = tigris::states(class = 'sf',year = '2017')
county_tigris$STATE_NAME = state_tigris$NAME[match(county_tigris$STATEFP,state_tigris$STATEFP)]
county_tigris$NAME = ifelse(county_tigris$NAME != county_tigris$STATE_NAME,county_tigris$NAME,paste(county_tigris$NAME,'County'))
clusterEvalQ(cl,{require(spacyr);require(data.table);require(stringr);
model = 'en_core_web_sm'
spacyr::spacy_initialize(model)
#text_storage = 'input/filtered_text_files/'
#ent_loc = 'scratch/boilerplate/project_place_entiities/'
})
gc()
ngaz = fread('../bucket_mount/tuolumne/scratch/NationalFile_20191101.txt',sep = '|',fill = T,stringsAsFactors = F,quote = "")
ngaz$CFIPS = ifelse(!is.na(ngaz$COUNTY_NUMERIC),paste0(formatC(ngaz$STATE_NUMERIC,width = 2,flag = '0'),formatC(ngaz$COUNTY_NUMERIC,width = 3,flag = '0')),NA)
ngaz = ngaz[!ngaz$FEATURE_NAME %in% c('Pacific Ocean','South Pacific Ocean','Atlantic Ocean','Gulf of Mexico','Caribbean Sea'),]
ngaz = ngaz[!is.na(ngaz$FEATURE_CLASS),]
ngaz = ngaz[ngaz$STATE_ALPHA %in% c(state.abb,'DC','PR'),]
ngaz = ngaz[!is.na(ngaz$COUNTY_NUMERIC),]
drop_types = c('Tower')
ngaz = ngaz[!ngaz$FEATURE_CLASS %in% drop_types,]
ngaz$COUNTY_NAME = county_tigris$NAME[match(ngaz$CFIPS,county_tigris$GEOID)]
ngaz = ngaz[ngaz$CFIPS%in% county_tigris$GEOID,]
ngaz$STATE_NAME = state_tigris$NAME[match(ngaz$STATE_ALPHA,state_tigris$STUSPS)]
ngaz = ngaz[ngaz$STATE_NAME != ngaz$FEATURE_NAME,]
ngaz = ngaz[!ngaz$FEATURE_NAME %in% c('River','Wilderness','Forest','Lake','Field','Beach','Mountain','Pacific','Atlantic','Earth','Acres','North','South','East','West',state.name),]
gc()
ngaz$FEATURE_NAME2 = str_replace_all(ngaz$FEATURE_NAME,pattern = '\\s','_')
fls = unique(top40pages$File)
geo_file = '../bucket_mount/tuolumne/scratch/boilerplate/geo_temp.rds'
if(file.exists(geo_file)){existing_geo = readRDS(geo_file)}else{existing_geo = NULL}
multis = projects[projects$State=='Multi',]
existing_geo = existing_geo[!PROJECT_ID %in% multis$PROJECT_ID,]
fls = fls[!fls %in% existing_geo$FILE]
text_sets = split(top40pages$text[top40pages$File %in% fls],top40pages$File[top40pages$File %in% fls])
fips_matches = foreach(i = text_sets,f = names(text_sets)) %dopar% {
id = str_remove(f,'_.*')
text = i
pars = spacy_parse(text,entity = T,dependency = T,full_parse = TRUE,lemma = F,pos = F,nounphrase = T)
ents = entity_consolidate(pars)
filter_ents = ents[ents$entity_type%in%c('GPE','LOC',"FAC"),]
if(nrow(filter_ents)>0){filter_ents$PROJECT_ID = id;filter_ents$FILE = f}
filter_ents
}
require(pbapply)
filtered_fips_matches = pblapply(fips_matches,function(filter_ents) {
states_proj = projects$State.or.Territory[unique(filter_ents$PROJECT_ID) == projects$EIS.Number]
states_proj = unlist(str_split(states_proj,'-'))
if(all(states_proj %in% state.abb)){sab = states_proj}else{
states = unique(filter_ents$token[filter_ents$token %in% gsub('\\s','_',state.name)])
if(length(states)==0){states = gsub("'",'',unique(filter_ents$token[gsub("'",'',filter_ents$token) %in% state.name]))}
sab = state.abb[str_replace(state.name,'\\s','_') %in% states]
}
ngaz_filter = ngaz[ngaz$STATE_ALPHA %in% sab,]
found_ents = filter_ents[filter_ents$token %in% ngaz_filter$FEATURE_NAME2,]
if(nrow(found_ents)>0){
found_ents$CFIPS <- ngaz_filter$CFIPS[match(found_ents$token,ngaz_filter$FEATURE_NAME2)]
found_ents$FEATURE_ID <- ngaz_filter$FEATURE_ID[match(found_ents$token,ngaz_filter$FEATURE_NAME2)]
found_ents$PRIM_LONG_DEC <- ngaz_filter$PRIM_LONG_DEC[match(found_ents$token,ngaz_filter$FEATURE_NAME2)]
found_ents$PRIM_LAT_DEC <- ngaz_filter$PRIM_LAT_DEC[match(found_ents$token,ngaz_filter$FEATURE_NAME2)]
}
found_ents
},cl=cors)
new_geomatches = rbindlist(filtered_fips_matches,fill =T,use.names = T)
new_geomatches[,STATEFP := str_extract(CFIPS,'^[0-9]{2}')]
matches_by_state_project = new_geomatches[,.N,by = .(PROJECT_ID,STATEFP)]
matches_by_state_project$STATE.ABB = fips.state$STUSAB[match(matches_by_state_project$STATEFP,formatC(fips.state$STATE,width = 2,flag = '0'))]
half_of_max = matches_by_state_project[,max(N)/2,by = .(PROJECT_ID)]
matches_by_state_project$half_of_max = half_of_max$V1[match(matches_by_state_project$PROJECT_ID,half_of_max$PROJECT_ID)]
projects$state_abb_list = str_split(projects$State.or.Territory,'-')
matches_by_state_project$listed_state = as.vector(mapply(function(x,y) x %in% y,x = matches_by_state_project$STATE.ABB,projects$state_abb_list[match(matches_by_state_project$PROJECT_ID,projects$EIS.Number)],SIMPLIFY = T))
good_states = matches_by_state_project[N>=half_of_max|listed_state,]
new_geomatches = new_geomatches[paste(PROJECT_ID,STATEFP) %in% paste(good_states$PROJECT_ID,good_states$STATEFP),]
full_geo = rbind(existing_geo,new_geomatches,use.names = T,fill = T)
full_geo = full_geo[PRIM_LAT_DEC!=0,]
saveRDS(full_geo,geo_file)
geomatches = readRDS(geo_file)
projects = fread('../bucket_mount/tuolumne/scratch/boilerplate/project_candidates_eis_only.csv')
projects = projects[Document=='Final',]
projects = projects[grepl('^201[3-9]|^2020',PROJECT_ID),]
projects = projects[!duplicated(EIS.Number),]
documents = fread( '../bucket_mount/tuolumne/scratch/boilerplate/document_candidates_eis_only.csv')
documents = documents[PROJECT_ID %in% projects$PROJECT_ID,]
have_docs = projects$EIS.Number[projects$EIS.Number %in% str_remove(full_tlist$File,'_.*')]
have_geo = projects$EIS.Number[projects$EIS.Number %in% geomatches$PROJECT_ID]
keep_fips = state_tigris$STATEFP[!state_tigris$STUSPS%in%c('AK','HI','GU','MU','PR','VI','AS','MP')]
require(ggthemes)
ggplot() + ggtitle('Geotagged locations in EIS documents','(excluding AK and HI)') +
geom_sf(data = state_tigris[state_tigris$STATEFP %in% keep_fips,],fill = 'white') +
theme_map() +
geom_point(pch = 21,data = geomatches[str_extract(geomatches$CFIPS,'^[0-9]{2}') %in% keep_fips,],aes(x = PRIM_LONG_DEC,y = PRIM_LAT_DEC),alpha = 0.15)
ggplot() + ggtitle('Geotagged locations in EIS documents','(excluding AK and HI)') +
geom_sf(data = state_tigris[state_tigris$STATEFP %in% c('40','48'),],fill = 'white') +
theme_map() +
geom_point(pch = 21,data = geomatches[PROJECT_ID == '20170215',],aes(x = PRIM_LONG_DEC,y = PRIM_LAT_DEC),alpha = 0.8)
geo_file
where = 'local'# or "remote"
rerun_existing = F
pack = c('data.table','data.table','quanteda','tm','textclean','stringr','pbapply',
'stringr','parallel','doParallel','benchmarkme','tidyverse','textreuse')
need = pack[!pack %in% installed.packages()[,'Package']]
lapply(need,install.packages)
lapply(pack,require,character.only=T)
cores = detectCores()
mcores = floor(detectCores()-15)
options("mc.cores" = floor(detectCores() -15))
#table(projects$AGENCY,projects$PROJECT_TYPE)
projects = fread('boilerplate_project/data_products/project_candidates_eis_only.csv')
dir.create('boilerplate_project/scratch/hash_candidates/')
if(where=='local'){scratch_loc = 'boilerplate_project/scratch/hash_candidates/'}
if(where == 'remote'){scratch_loc = '../bucket_mount/tuolumne/scratch/boilerplate/hash_candidates/'}
minhash <- minhash_generator(n = 240, seed = 40)
progress_bars = T
gc()
cluster = makeCluster(mcores)
registerDoParallel(cl = cluster,cores = mcores)
parallel::clusterEvalQ(cluster,'require(data.table)')
parallel::clusterEvalQ(cluster,'require(textreuse)')
#hash_file = paste0('../bucket_mount/big_eis_text.rds')
full_tlist <- readRDS('boilerplate_project/input/eis_corpus_2013-2020.rds')
full_tlist$text = gsub('\"\"','',full_tlist$text,fixed = T)
chars = nchar(full_tlist$text)
periods = stringr::str_count(full_tlist$text,"\\.")
numbers = stringr::str_count(full_tlist$text,"[0-9]")
caps = stringr::str_count(full_tlist$text,'[A-Z]')
tildes = stringr::str_count(full_tlist$text,'~')
quotes = stringr::str_count(full_tlist$text,'\\"')
spaces = stringr::str_count(full_tlist$text,'\\s')
cut = 0.1
full_tlist  = full_tlist[chars>400&{periods/chars}<cut&{quotes/chars}<cut&{tildes/chars}<cut&{numbers/chars}<cut&{caps/chars}<cut&{spaces/chars}<{cut*2},]
#####
hash_file = paste0(scratch_loc,'eis_page_hashes.rds')
flist = as.character(full_tlist$text)
names(flist) <- paste0(full_tlist$File,'_',full_tlist$Page)
summary(nchar(full_tlist$text))
eis_corpus =  TextReuseCorpus(text = flist,
meta = list(File = full_tlist$File,Page = full_tlist$Page),
tokenizer = tokenize_ngrams, n = 10,
minhash_func = minhash, keep_tokens = TRUE,
progress = progress_bars,skip_short = T)
#eis_corpus = readRDS('../bucket_mount/tuolumne/scratch/eis_page_corp_scratch.rds')
#file.exists('../bucket_mount/tuolumne/scratch/eis_page_corp_scratch.rds')
split_corpus_ntiles = dplyr::ntile(x = seq(eis_corpus),n = mcores*10)
split_corpus = split(eis_corpus,split_corpus_ntiles)
rm(eis_corpus)
gc()
split_buckets = foreach(x = split_corpus) %dopar% {textreuse::lsh(x,bands = 60)}
gc()
#
while(any(sapply(split_buckets,is.null))){
null_fails = which(sapply(split_buckets,is.null))
split_buckets[null_fails] <- pblapply(null_fails,function(x) lsh(split_corpus[[x]],bands = 60,progress = F),cl = 5)
}
eis_buckets = do.call(rbind,split_buckets)
#saveRDS(eis_buckets,'../bucket_mount/tuolumne/scratch/eis_page_buckets_scratch.rds')
#eis_buckets = readRDS('../bucket_mount/tuolumne/scratch/eis_page_buckets_scratch.rds')
eis_candidates <- lsh_candidates(buckets = eis_buckets)
require(dplyr)
candidate_splits = split(eis_candidates,ntile(1:nrow(eis_candidates),n = nrow(eis_candidates) %/% 10000))
gc()
#corpus_meta = eis_corpus$meta
#corpus_meta$names <- names(eis_corpus)[!names(eis_corpus)%in%skipped(eis_corpus)]
require(doParallel)
score_list = foreach(i = candidate_splits) %dopar% {
send_names = unique(c(i$a,i$b))
send_text = flist[send_names]
score_list = mapply(function(aa,bb) textreuse::align_local(a = aa,b=bb)$score,aa = send_text[i$a],bb = send_text[i$b])
data.table::data.table(a = i$a, b= i$b,score = score_list)
}
score_dt = rbindlist(score_list)
saveRDS(score_dt,'boilerplate_project/data_products/eis_page_scores_scratch.rds')
##### A FEW STEPS HERE, PARTICULARLY THE CORPUS GENERATION, ARE EXTREMELY COMPUTATIONALLY INTENSIVE
##### THIS WAS RUN ON A GOOGLE VIRTUAL MACHINE WITH 16 CORES AND 360 GB OF RAM (MEMORY IS THE LIMITING FACTOR, ADDING MORE CORES DOES NOT HELP)
##### IN ADDITION, IF MEMORY GETS TOO TIGHT, THE CORPUS GENERATION WILL APPEAR TO WORK, BUT A MESSAGE WILL OCCUR THAT SAYS A BUNCH (E.G. 1M OR 100K) OF THE TEXTS
# WERE TOO SHORT AND THUS DROPPED. IF THIS OCCURS, IT MEANS THE CODE DID NOT WORK RIGHT AND SHOULD BE RUN AGAIN.
where = 'remote'# or "remote"
rerun_existing = F
pack = c('data.table','data.table','quanteda','tm','textclean','stringr','pbapply',
'stringr','parallel','doParallel','benchmarkme','tidyverse','textreuse')
need = pack[!pack %in% installed.packages()[,'Package']]
lapply(need,install.packages)
lapply(pack,require,character.only=T)
#table(projects$AGENCY,projects$PROJECT_TYPE)
projects = fread('boilerplate_project/data_products/project_candidates_eis_only.csv')
#dir.create('boilerplate_project/scratch/hash_candidates/')
#if(where=='local'){
scratch_loc = 'boilerplate_project/scratch/hash_candidates/'#}
#if(where == 'remote'){scratch_loc = '../bucket_mount/tuolumne/scratch/boilerplate/hash_candidates/'}
minhash <- minhash_generator(n = 240, seed = 40)
progress_bars = T
gc()
#hash_file = paste0('../bucket_mount/big_eis_text.rds')
full_tlist <- readRDS('boilerplate_project/input/eis_corpus_2013-2020.rds')
full_tlist[,text:=gsub('\"\"','',text,fixed = T)]
chars = nchar(full_tlist$text)
periods = stringr::str_count(full_tlist$text,"\\.")
numbers = stringr::str_count(full_tlist$text,"[0-9]")
caps = stringr::str_count(full_tlist$text,'[A-Z]')
tildes = stringr::str_count(full_tlist$text,'~')
quotes = stringr::str_count(full_tlist$text,'\\"')
spaces = stringr::str_count(full_tlist$text,'\\s')
cut = 0.1
full_tlist  = full_tlist[chars>400&{periods/chars}<cut&{quotes/chars}<cut&{tildes/chars}<cut&{numbers/chars}<cut&{caps/chars}<cut&{spaces/chars}<{cut*2},]
#####
hash_file = paste0(scratch_loc,'eis_page_hashes.rds')
flist = as.character(full_tlist$text)
names(flist) <- paste0(full_tlist$File,'_',full_tlist$Page)
summary(nchar(full_tlist$text))
gc()
mcores = 2#floor(detectCores()/2)
#options("mc.cores" = floor(detectCores()/2))
options("mc.cores" = mcores)
eis_corpus =  TextReuseCorpus(text = flist,
meta = list(File = full_tlist$File,Page = full_tlist$Page),
tokenizer = tokenize_ngrams, n = 10,
minhash_func = minhash, keep_tokens = TRUE,
progress = progress_bars,skip_short = T)
##### A FEW STEPS HERE, PARTICULARLY THE CORPUS GENERATION, ARE EXTREMELY COMPUTATIONALLY INTENSIVE
##### THIS WAS RUN ON A GOOGLE VIRTUAL MACHINE WITH 16 CORES AND 360 GB OF RAM (MEMORY IS THE LIMITING FACTOR, ADDING MORE CORES DOES NOT HELP)
##### IN ADDITION, IF MEMORY GETS TOO TIGHT, THE CORPUS GENERATION WILL APPEAR TO WORK, BUT A MESSAGE WILL OCCUR THAT SAYS A BUNCH (E.G. 1M OR 100K) OF THE TEXTS
# WERE TOO SHORT AND THUS DROPPED. IF THIS OCCURS, IT MEANS THE CODE DID NOT WORK RIGHT AND SHOULD BE RUN AGAIN.
where = 'remote'# or "remote"
rerun_existing = F
pack = c('data.table','data.table','quanteda','tm','textclean','stringr','pbapply',
'stringr','parallel','doParallel','benchmarkme','tidyverse','textreuse')
need = pack[!pack %in% installed.packages()[,'Package']]
lapply(need,install.packages)
lapply(pack,require,character.only=T)
#table(projects$AGENCY,projects$PROJECT_TYPE)
projects = fread('boilerplate_project/data_products/project_candidates_eis_only.csv')
#dir.create('boilerplate_project/scratch/hash_candidates/')
#if(where=='local'){
scratch_loc = 'boilerplate_project/scratch/hash_candidates/'#}
#if(where == 'remote'){scratch_loc = '../bucket_mount/tuolumne/scratch/boilerplate/hash_candidates/'}
minhash <- minhash_generator(n = 240, seed = 40)
progress_bars = T
gc()
#hash_file = paste0('../bucket_mount/big_eis_text.rds')
full_tlist <- readRDS('boilerplate_project/input/eis_corpus_2013-2020.rds')
full_tlist[,text:=gsub('\"\"','',text,fixed = T)]
p0 = proc.time()
p0
proc.time()
# Start the clock!
ptm <- proc.time()
chars = nchar(full_tlist$text)
# Stop the clock
proc.time() - ptm
# Start the clock!
ptm <- proc.time()
periods = stringr::str_count(full_tlist$text,"\\.")
# Stop the clock
proc.time() - ptm
# Start the clock!
ptm <- proc.time()
periods = pbsapply(full_tlist$text,stringr::str_count,pattern = "\\.",cl = 30)
