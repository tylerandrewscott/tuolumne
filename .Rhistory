if(!require(ggthemes)){install.packages('ggthemes');library(ggthemes)}
if(!require(scales)){install.packages('scales');library(scales)}
if(!require(R2HTML)){install.packages('R2HTML');library(R2HTML)}
if(!require(sf)){install.packages('sf');library(sf)}
td = tempdir()
albersNA <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-110 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m"
require(data.table)
require(stringr)
timber = fread('~/Downloads/Timber_Harvests__Feature_Layer_.csv')
timber = timber
#timber = timber[FY_AWARDED>=2004,]
#timber_nepa = timber[timber$NEPA_PROJECT_ID!='NOT REQD',]
timber$FOREST_ID = paste0(formatC(timber$ADMIN_REGION_CODE,width = 2, flag = 0),formatC(timber$ADMIN_FOREST_CODE,width = 2,flag = 0))
timber$NEPA_PROJECT_ID <- as.integer(timber$NEPA_PROJECT_ID)
timber$NEPA_PROJECT_ID[!is.na(timber$NEPA_PROJECT_ID)]<- formatC(timber$NEPA_PROJECT_ID[!is.na(timber$NEPA_PROJECT_ID)],width = 8,flag = 0)
#test=timber[timber$FY_AWARDED=='2014'&!timber$NEPA_PROJECT_ID %in% pals$`PROJECT NUMBER`,]
fs = fread('https://docs.google.com/spreadsheets/d/e/2PACX-1vRuz53_guoKQejdd97Syvws360-S7g21tkjeH73OJ1K6zWKNlGwaDRLvd7bbwIoyLmYCf9RWsAzETu-/pub?output=csv',stringsAsFactors = F)
fs$ongoing = 0
fs2 = fread('https://docs.google.com/spreadsheets/d/e/2PACX-1vQgOk_TxujlSfspmA9-V4sBS95AMVu7MbjyCmbsIc1yu5N983sJqoR8usHvOXpMl99d1yBYejWoZtdk/pub?output=csv',stringsAsFactors = F)
numcols = colnames(fs2)[as.vector(apply(fs2,2,function(x) !any(grepl('[^0-9]',x))))]
fs2[,(numcols):=lapply(.SD,as.numeric),.SDcols = numcols]
fs = rbind(fs,fs2,use.names = T,fill = T)
fs = fs[!duplicated(paste(`PROJECT NUMBER`,`LMU (ACTUAL)`)),]
#fs = fs2
#fs = fs[fs$`UNIQUE DECISION?`=='Y',]
fs$`LMU (ACTUAL)`[fs$`LMU (ACTUAL)`=="Buffalo Ranger District (11040306)"] <- "Blackrock Ranger District (11040306)"
fs$`LMU (ACTUAL)`[fs$`LMU (ACTUAL)`=="Powell Ranger District  (11010506)"] <- "Lochsa/Powell Ranger District (11011755)"
fs$`LMU (ACTUAL)`[fs$`LMU (ACTUAL)`=="Salmon River Ranger District (11050554)"] <- "Salmon-Scott Ranger District (11050554)"
fs$`LMU (ACTUAL)`[fs$`LMU (ACTUAL)`=="Paintrock Ranger District (11020204)"] <- "Salmon-Scott Ranger District (11020203)"
fs$`LMU (ACTUAL)`[fs$`LMU (ACTUAL)`=="Belt Creek Ranger District (11011503)"] <- "Belt Creek-White Sulphur Springs Ranger District (11011507)"
fs$`LMU (ACTUAL)`[fs$`LMU (ACTUAL)`=="Ashton/Island Park (11041552)" ]<- "Ashton/Island Park Ranger District (11041552)"
fs$`LMU (ACTUAL)`[fs$`LMU (ACTUAL)`=="Los Angeles River (11050151)" ]<- "Los Angeles River Ranger District (11050151)"
fs$`LMU (ACTUAL)`[fs$`LMU (ACTUAL)`=="Santa Clara/Mojave Rivers (11050153)"  ]<- "Santa Clara/Mojave Rivers Ranger District (11050153)"
fs$UNIT_ID = str_extract(str_extract(fs$`LMU (ACTUAL)`,'[0-9]{3,}'),'[0-9]{6}$')
fs$UNIT_ID = gsub('^0108','0111',fs$UNIT_ID)
fs$UNIT_ID = gsub('^0105','0117',fs$UNIT_ID)
fs$UNIT_ID = gsub('^0112','0115',fs$UNIT_ID)
fs$UNIT_ID[fs$UNIT_ID=='011702'] <- '011752'
fs$UNIT_ID[fs$UNIT_ID=='011703'] <- '011753'
fs$UNIT_ID[fs$UNIT_ID=='011501'] <- '011511'
fs$UNIT_ID[fs$UNIT_ID=='011504'] <- '011514'
fs$UNIT_ID[fs$UNIT_ID=='011104'] <- '011184'
fs$UNIT_ID[fs$UNIT_ID=='011502'] <- '011512'
fs$UNIT_ID[fs$UNIT_ID=='011102'] <- '011182'
fs$UNIT_ID[fs$UNIT_ID=='011706'] <- '011755'
fs$FOREST_ID <- str_extract(fs$UNIT_ID,'^[0-9]{4}')
fs$REGION_ID <- str_extract(fs$UNIT_ID,'^[0-9]{2}')
fs$`PROJECT NUMBER`<- formatC(fs$`PROJECT NUMBER`,width = 8,flag = 0)
#fs = fs[fs$`DECISION TYPE`!='PAD',]
pals = fs
#pals = pals[pals$`INITIATION FY`>=2004,]
#pal
temp[NEPA_DOC_NAME=="CANYON CREEK RESEARCH PROJECT EA",]
pals[`PROJECT NUMBER`=='00098805',]
temp$NEPA_DOC_NAME
temp$NEPA_DOC_NAME[temp$FY_PLANNED==2018]
temp[NEPA_DOC_NAME=="(PALS)MT. HULL RESTORATION PROJECT" ]
pals[`PROJECT NUMBER`=='00056466',]
temp[NEPA_DOC_NAME=="(PALS)MT. HULL RESTORATION PROJECT" ]
pals[`PROJECT NUMBER`=='00051107',]
pals[grep('HULL',toupper(pals$`PROJECT NAME`)),][FOREST_ID=='0617',]
library(reticulate)
library(purrr)
library(text2vec)
library(dplyr)
library(Rtsne)
library(ggplot2)
library(plotly)
library(stringr)
library(tensorflow)
library(keras)
tf$constant("Hellow Tensorflow")
install_tensorflow()
install_tensorflow(method = "conda")
which conda
where conda
use_condaenv('/opt/anaconda3/bin/conda')
install_tensorflow(method = "conda")
install_tensorflow(conda = '/opt/anaconda3/bin/conda')
system('which python')
system('which conda')
system('where conda')
system('which conda')
Sys.setenv(RETICULATE_PYTHON="/opt/anaconda3/bin/conda")
library(keras)
install_keras()
Sys.setenv(RETICULATE_PYTHON="~/opt/anaconda3/bin/conda")
library(keras)
install_keras()
system('which python3')
Sys.setenv(RETICULATE_PYTHON="/usr/bin/python3")
library(reticulate)
library(purrr)
library(text2vec)
library(dplyr)
library(Rtsne)
library(ggplot2)
library(plotly)
library(stringr)
library(tensorflow)
library(keras)
tf
tf$constant("Hellow Tensorflow")
reticulate::use_python("/usr/bin/python3")
tf$constant("Hellow Tensorflow")
reticulate::py_discover_config()
fread('Downloads/16incyallagi.csv')
require(data.table)
fread('Downloads/16incyallagi.csv')
dt = fread('Downloads/16incyallagi.csv')
require('fastTextR')
install.packages('fastTextR')
require('fastTextR')
## Not run:
flist = list.files('Documents/GitHub/tuolumne/scratch/full_text_documents/',pattern = '^2014',full.names = T)
flist
grepl('^([0-9]{8})_\\1\\.txt',flist)
table(grepl('^([0-9]{8})_\\1\\.txt',flist))
table(grepl('^([0-9]{8})_\\1',flist))
table(grepl('([0-9]{8})_\\1',flist,perl = T))
table(grepl('([0-9]{8})_\\1\\.txt$',flist,perl = T))
flist = flist[!grepl('([0-9]{8})_\\1\\.txt$',flist,perl = T)]
flist
cntrl <- ft_control(nthreads = 1L)
model <- ft_train(flist[10], method="cbow", control = cntrl)
model
cntrl <- ft_control(nthreads = 4L,epoch = 5,max_ngram = 2,
min_count = 5,window_size = 5,word_vec_size = 50)
model <- ft_train(flist[10], method="cbow", control = cntrl)
model
summary(model)
ft_word_vectors(model,words = 'water_quality')
ft_word_vectors(model,words = 'water')
ft_word_vectors(model,words = c('water','air','soil'))
ft_word_vectors(model,words = c('air pollution'))
ft_word_vectors(model,words = c('air_pollution'))
model$nwords
model$nlabels
model$words()
?fastTextR::ft_normalize()
fastTextR::ft_normalize(flist[10])
txt = readLines(flist[10])
str(txt)
length(txt)
nchar(txt)
norm_txt = fastTextR::ft_normalize(txt)
str(txt)
str(norm_txt)
ft <- fasttext()
library("fastTextR")
model <- ft_load("cc.en.300.bin")
ft <- fasttext()
ft
cntrl <- ft_control(nthreads = 4L,epoch = 5,max_ngram = 2,
min_count = 5,window_size = 5,word_vec_size = 50)
ft$train(file = flist[10],method = 'cbow',control = cntl)
ft$train(file = flist[10],method = 'cbow',control = cntrl)
ft$words()
getwd()
setwd('Documents/GitHub/tuolumne/')
require(here)
install.packages('here')
require(here)
here()
here()
setwd(here('GitHub/tuolumne'))
setwd('../')
getwd()
setwd('../../')
getwd()
setwd(here('GitHub/tuolumne'))
here('GitHub/tuolumne')
setwd(here('GitHub','tuolumne'))
getwd()
require(here)
setwd(here('GitHub','tuolumne'))
setwd(here('tuolumne'))
here('GitHub/tuolumne')
setwd(here('GitHub/tuolumne'))
requrie(here)
require(here)
here()
here('tuolumne')
here('Github/tuolumne')
here('Documents/Github/tuolumne')
here('Github','tuolumne')
library(spacyr)
system('which python3')
system('which conda')
system('where conda')
spacy_initialize(model = 'en_core_web_sm',python_executable = '/opt/anaconda3/bin/python3')
spacy_parse('tyler went to the store to watch basketball')
spacy_initialize(model = 'en_core_web_lg',python_executable = '/opt/anaconda3/bin/python3')
cd_ideology = readRDS('Documents/GitHub/manitou/input/politics_data/final/cd_ideology_8-2020.RDS')
cd_ideology = readRDS('Box/manitou/input/politics_data/final/cd_ideology_8-2020.RDS')
cd_ideology = readRDS('Box/manitou/input/politics_data/final/cd_ideology_8-2020.RDS')
cd_ideology
cd_ideology = readRDS('Box/manitou/input/politics_data/final/countyVoteShare_3-2020.rds')
cd_ideology
cd_ideology = readRDS('Box/manitou/input/politics/countyVoteShare_3-2020_imputed.rds')
cd_ideology
library(spacyr)
install.packages(c("data.table", "devtools", "DT"))
library(spacyr)
remove.packages("spacyr")
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
library("spacyr")
spacy_install()
spacy_initialize()
setwd('Documents/GitHub/tuolumne/')
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
new_floc = 'scratch/tokenized_paragraphs/'
dir.create(new_floc)
FindCleanParagraphs = function(file,dir){
temp=readtext::readtext(paste0(temp_doc_loc,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(dir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
}
require(doParallel)
cores = 4
cl = makeCluster(cores)
doParallel::registerDoParallel(cl)
clusterExport(cl,varlist = list('FindCleanParagraphs'))
flist = flist[!file.exists(paste0(new_floc,gsub('pdf$|PDF$','txt',basename(flist))))]
foreach(f = flist[1:100]) %dopar% {FindCleanParagraphs(f,dir = new_floc)}
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
new_floc = 'scratch/tokenized_paragraphs/'
dir.create(new_floc)
FindCleanParagraphs = function(file,dir){
temp=readtext::readtext(paste0(text_loc,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(dir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
}
require(doParallel)
cores = 4
cl = makeCluster(cores)
doParallel::registerDoParallel(cl)
clusterExport(cl,varlist = list('FindCleanParagraphs'))
flist = flist[!file.exists(paste0(new_floc,gsub('pdf$|PDF$','txt',basename(flist))))]
foreach(f = flist[1:100]) %dopar% {FindCleanParagraphs(f,dir = new_floc)}
flist
file = "20150287_DRAFT_EIS_Next_NGA_West_Campus_Greater_StLouis_Area.txt"
temp=readtext::readtext(paste0(text_loc,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(dir,basename(file))
dir = new_floc
new_file = paste0(dir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
close(fileConn)
if(length(filtered_pars)>0){
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
dir = new_floc
file = "20150287_DRAFT_EIS_Next_NGA_West_Campus_Greater_StLouis_Area.txt"
temp=readtext::readtext(paste0(text_loc,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(dir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
temp
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
new_floc = 'scratch/tokenized_paragraphs/'
dir.create(new_floc)
FindCleanParagraphs = function(file,dir){
temp=readtext::readtext(paste0(text_loc,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(dir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
file.create(new_file)
fileConn<-file(new_file)
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
}
require(doParallel)
cores = 4
cl = makeCluster(cores)
doParallel::registerDoParallel(cl)
clusterExport(cl,varlist = list('FindCleanParagraphs'))
flist = flist[!file.exists(paste0(new_floc,gsub('pdf$|PDF$','txt',basename(flist))))]
foreach(f = flist[1:100]) %dopar% {FindCleanParagraphs(f,dir = new_floc)}
flist[1]
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
new_floc = 'scratch/tokenized_paragraphs/'
dir.create(new_floc)
FindCleanParagraphs = function(file,newdir,olddir){
temp=readtext::readtext(paste0(olddir,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(newdir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
file.create(new_file)
fileConn<-file(new_file)
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
}
stopCluster(cl)
require(doParallel)
cores = 4
cl = makeCluster(cores)
doParallel::registerDoParallel(cl)
clusterExport(cl,varlist = list('FindCleanParagraphs','text_loc','new_floc'))
flist = flist[!file.exists(paste0(new_floc,gsub('pdf$|PDF$','txt',basename(flist))))]
foreach(f = flist[1:100]) %dopar% {FindCleanParagraphs(file = f,olddir = text_loc,newdir = new_floc)}
f = flist[1]
FindCleanParagraphs(file = f,olddir = text_loc,newdir = new_floc)
FindCleanParagraphs = function(file,newdir,olddir){
temp=readtext::readtext(paste0(olddir,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(newdir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
file.create(new_file)
fileConn<-file(new_file)
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
}
f
f = flist[1]
f
flist
new_floc
paste0(new_floc,gsub('pdf$|PDF$','txt',basename(flist)))
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
flist
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
fl = list.files(text_loc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
docs
require(stringr)
require(data.table)
require(pdftools)
flist= list.files('../eis_documents/enepa_repository/documents/',full.names = T,recursive = T)
flist = flist[grepl('^2020|^201[3-9]',basename(flist))]
flist = flist[grepl('PDF$|pdf$',flist)]
floc = 'scratch/full_text_documents/'
dir.create(floc)
require(parallel)
concatenate_save = function(file){
new_file = paste0(floc,gsub('PDF$|pdf$','txt',basename(file)))
print(new_file)
try = tryCatch(pdf_info(file),error = function(e) NULL)
if(!is.null(try)){
tv = pdftools::pdf_text(file)
temp = paste(tv,collapse = ' ')
if(nchar(temp)>0){
if(any(grepl('[A-Za-z]',temp))){
file.create(new_file)
fileConn<-file(new_file)
writeLines(text = temp, fileConn)
close(fileConn)
}
}
}
}
require(doParallel)
cores = 4
cl = makeCluster(cores)
doParallel::registerDoParallel(cl)
clusterExport(cl,varlist = list('concatenate_save'))
fl = list.files(floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = flist[!file.exists(paste0(floc,gsub('pdf$|PDF$','txt',basename(flist))))]
foreach(f = rev(flist)) %do% {concatenate_save(f)}
1.249e-04
