chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(dir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
file.create(new_file)
fileConn<-file(new_file)
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
}
require(doParallel)
cores = 4
cl = makeCluster(cores)
doParallel::registerDoParallel(cl)
clusterExport(cl,varlist = list('FindCleanParagraphs'))
flist = flist[!file.exists(paste0(new_floc,gsub('pdf$|PDF$','txt',basename(flist))))]
foreach(f = flist[1:100]) %dopar% {FindCleanParagraphs(f,dir = new_floc)}
flist[1]
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
new_floc = 'scratch/tokenized_paragraphs/'
dir.create(new_floc)
FindCleanParagraphs = function(file,newdir,olddir){
temp=readtext::readtext(paste0(olddir,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(newdir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
file.create(new_file)
fileConn<-file(new_file)
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
}
stopCluster(cl)
require(doParallel)
cores = 4
cl = makeCluster(cores)
doParallel::registerDoParallel(cl)
clusterExport(cl,varlist = list('FindCleanParagraphs','text_loc','new_floc'))
flist = flist[!file.exists(paste0(new_floc,gsub('pdf$|PDF$','txt',basename(flist))))]
foreach(f = flist[1:100]) %dopar% {FindCleanParagraphs(file = f,olddir = text_loc,newdir = new_floc)}
f = flist[1]
FindCleanParagraphs(file = f,olddir = text_loc,newdir = new_floc)
FindCleanParagraphs = function(file,newdir,olddir){
temp=readtext::readtext(paste0(olddir,file))
pars = tokenizers::tokenize_paragraphs(temp$text,paragraph_break = '.\n',simplify = T)
pars <- stringr::str_replace_all(pars,'\\s{2,}',' ')
chars = nchar(pars)
periods = stringr::str_count(pars,"\\.")
numbers = stringr::str_count(pars,"[0-9]")
caps = stringr::str_count(pars,'[A-Z]')
spaces = stringr::str_count(pars,'\\s')
filtered_pars = pars[chars>400&{periods/chars}<0.1&{numbers/chars}<0.1&{caps/chars}<0.1&!grepl('http',pars)&{spaces/chars}<0.2]
new_file = paste0(newdir,basename(file))
print(new_file)
file.create(new_file)
fileConn<-file(new_file)
if(length(filtered_pars)>0){
file.create(new_file)
fileConn<-file(new_file)
writeLines(text = filtered_pars, fileConn)
close(fileConn)
}
}
f
f = flist[1]
f
flist
new_floc
paste0(new_floc,gsub('pdf$|PDF$','txt',basename(flist)))
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
flist
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm','tokenizers')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata.RDS')
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
text_loc = 'scratch/full_text_documents/'
fl = list.files(text_floc,full.names = T)
fl = list.files(text_loc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = list.files(text_loc)
flist = flist[flist %in% gsub('pdf$','txt',docs$File_Name)]
docs
require(stringr)
require(data.table)
require(pdftools)
flist= list.files('../eis_documents/enepa_repository/documents/',full.names = T,recursive = T)
flist = flist[grepl('^2020|^201[3-9]',basename(flist))]
flist = flist[grepl('PDF$|pdf$',flist)]
floc = 'scratch/full_text_documents/'
dir.create(floc)
require(parallel)
concatenate_save = function(file){
new_file = paste0(floc,gsub('PDF$|pdf$','txt',basename(file)))
print(new_file)
try = tryCatch(pdf_info(file),error = function(e) NULL)
if(!is.null(try)){
tv = pdftools::pdf_text(file)
temp = paste(tv,collapse = ' ')
if(nchar(temp)>0){
if(any(grepl('[A-Za-z]',temp))){
file.create(new_file)
fileConn<-file(new_file)
writeLines(text = temp, fileConn)
close(fileConn)
}
}
}
}
require(doParallel)
cores = 4
cl = makeCluster(cores)
doParallel::registerDoParallel(cl)
clusterExport(cl,varlist = list('concatenate_save'))
fl = list.files(floc,full.names = T)
sizes = file.size(fl)
file.remove(fl[sizes==0])
flist = flist[!file.exists(paste0(floc,gsub('pdf$|PDF$','txt',basename(flist))))]
foreach(f = rev(flist)) %do% {concatenate_save(f)}
1.249e-04
packs = c('stm','data.table','stringr','readtext','pbapply','maps','quanteda','tm')
sapply(packs[!sapply(packs,require,character.only = T)],install.packages)
sapply(packs,require)
redraw_corpus = TRUE
projs = readRDS('scratch/climate_in_nepa/eis_metadata_with_covariates.RDS')
projs = data.table(left_join(projs,projs[,list(mean(skills_rating),mean(ideo_rating)),by=.(AGENCY)]))
projs$ABBREV[projs$AGENCY=='Nuclear Regulatory Commission'] <- 'NRC'
projs$ABBREV[projs$AGENCY=='Department of Defense'] <- 'DoD'
projs$ABBREV[projs$AGENCY=='Department of Commerce'] <- 'DoC'
projs$ABBREV[projs$AGENCY=='Bureau of Indian Affairs'] <- 'BIA'
projs$ABBREV[projs$AGENCY=='Department of Housing and Urban Development'] <- 'HUD'
projs$ABBREV[projs$AGENCY=='U.S. Army Corps of Engineers'] <- 'ACOE'
projs$ABBREV[projs$AGENCY=='Federal Highway Administration'] <- 'FHWA'
projs$ABBREV[projs$AGENCY=='Federal Energy Regulatory Commission'] <- 'FERC'
projs$ABBREV[projs$AGENCY=='General Services Administration'] <- 'GSA'
projs$ABBREV[projs$AGENCY=='Department of Health and Human Services'] <- 'HHS'
projs$ABBREV[projs$AGENCY=='Department of Homeland Security'] <- 'DHS'
projs$ABBREV[projs$AGENCY=="Fish and Wildlife Service"  ] <- 'FWS'
projs$ABBREV[projs$AGENCY=="National Park Service"  ] <- 'NPS'
projs$ABBREV[projs$AGENCY=="Forest Service"  ] <- 'FS'
projs$ABBREV[projs$AGENCY=="Bureau of Reclamation" ] <- 'BR'
projs$ABBREV[projs$AGENCY=="Department of Interior (other)" ] <- 'DoI (other)'
projs$ABBREV[projs$AGENCY=="USDA (non-FS)" ] <- 'DoA (other)'
projs$ABBREV[projs$AGENCY=="Department of Transportation (other)" ] <- 'DoT (other)'
projs$ABBREV[projs$AGENCY=="Tennessee Valley Authority"  ] <- 'TVA'
projs$ABBREV[projs$AGENCY=="Bureau of Land Management" ] <- 'BLM'
projs$ABBREV[projs$AGENCY=="Department of Energy" ] <- 'DoE'
projs$ABBREV[projs$AGENCY== "National Oceanic and Atmospheric Administration"  ] <- 'NOAA'
require(ggrepel)
projs[!duplicated(AGENCY),][,.(ABBREV,AGENCY,ideo_rating,skills_rating)]
ggplot(projs[!duplicated(AGENCY),]) +
geom_hline(yintercept = 0.67970574,lty=2)+
geom_label(x = -1,y = 0.67970574,label='TVA')+
geom_point(aes(x = ideo_rating,y = skills_rating)) +
geom_text_repel(aes(x = ideo_rating,y = skills_rating,label = ABBREV)) +
theme_bw() + xlab('Ideological rating (liberal < conservative)') + ylab('Workforce skill (less < more)') +
ggtitle('Ideology and workforce skill by agency','(using Richardson et al. 2018 scores)') +
labs(caption = "*TVA missing ideology score")
docs = readRDS('scratch/climate_in_nepa/eis_doc_metadata.RDS')
where_is_scratch = 'scratch/climate_in_nepa/'
require(lubridate)
projs$DEC_DATE = decimal_date(mdy(projs$Federal.Register.Date))-2013
quanteda_toks = list.files('scratch/climate_in_nepa/yearly_quanteda_tokens/',full.names = T)
tok_list = lapply(quanteda_toks,readRDS)
#do.call is good code, but takes _forever_
#tok_all =do.call("+",tok_list)
#this is bad code, but goes fast
tok_all = tok_list[[1]]
for(t in tok_list[-1]){
tok_all = tok_all + t
}
qdfm = dfm(tok_all)
qdfm_freq <- dfm_trim(qdfm, min_docfreq = 2)
qdfm_freq <- dfm_trim(qdfm_freq, max_docfreq = 0.05,docfreq_type = 'prop')
meta_eis = projs[match(str_remove(qdfm_freq@Dimnames$docs,'_.*'),projs$EIS.Number),]
meta_eis$EIS.Number <- as.character(meta_eis$EIS.Number)
meta_eis$ID = qdfm_freq@Dimnames$docs
K = 90
dfm2stm <- convert(qdfm_freq, to = "stm")
meta_eis_sub = meta_eis[ID %in% names(dfm2stm$documents),]
meta_eis_sub$YEAR = str_extract(meta_eis_sub$EIS.Number,'^[0-9]{4}')
dfm2stm$meta = meta_eis_sub
#use k = 0 to automate guess to understand range of k
# model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = K, data = dfm2stm$meta,
#                  prevalence = ~EIS.Number + PROJECT_TYPE + PROJECT_TOPIC + AGENCY + YEAR+
#                    s(skills_rating) + s(ideo_rating),
#                  init.type = "Spectral",verbose = T,
#                  seed = 24,max.em.its = 40,emtol = 0.0001)
# saveRDS(model.stm,paste0(where_is_scratch,'temp_stm_90k.RDS'))
model.stm = readRDS(paste0(where_is_scratch,'temp_stm_90k.RDS'))
theta_medians = sapply(1:90,function(t) {median(model.stm$theta[,t])})
mdt = make.dt(model.stm)
require(ggthemes)
frequency <- colMeans(model.stm$theta)
freq_dt = data.table(frequency,rank = rank(-frequency),topic = 1:90)
freq_dt = freq_dt[order(rank)]
ggplot(data = freq_dt) + ggtitle('Topics ordered by expected frequency')+
geom_bar(aes(x = rank,y = frequency,fill = ifelse(topic == c(11),'Topic 11',ifelse(topic==28,'Topic 28','Other'))),
stat = 'identity') +
coord_flip() +
scale_x_continuous(breaks = c(1:90),labels=freq_dt$topic,name = 'topic',expand=c(0,0))+ theme_bw() +
theme(axis.ticks = element_blank(),legend.title = element_blank(),
#axis.text.x = element_text(angle = 45),
legend.position = c(0.75,0.25)) + scale_fill_colorblind()
ggplot(data = freq_dt) + ggtitle('Topics ordered by expected frequency')+
geom_bar(aes(x = rank,y = frequency,fill = ifelse(topic == c(11),'Topic 11',ifelse(topic==28,'Topic 28','Other'))),
stat = 'identity') +
geom_label(aes(x = rank,y = frequency,label = topic))+
coord_flip() +
scale_x_continuous(breaks = c(1:90),labels=freq_dt$topic,name = 'topic',expand=c(0,0))+ theme_bw() +
theme(axis.ticks = element_blank(),legend.title = element_blank(),
#axis.text.x = element_text(angle = 45),
legend.position = c(0.75,0.25)) + scale_fill_colorblind()
source('~/Documents/GitHub/tuolumne/code/workflow_climate_in_nepa/test_stm.R', echo=TRUE)
ggplot(data = freq_dt) + ggtitle('Topics ordered by expected frequency')+
geom_bar(aes(x = rank,y = frequency,fill = ifelse(topic == c(11),'Topic 11',ifelse(topic==28,'Topic 28','Other'))),
stat = 'identity') +
geom_label_repel(aes(x = rank,y = frequency,label = topic))+
coord_flip() +
scale_x_continuous(breaks = c(1:90),labels=freq_dt$topic,name = 'topic',expand=c(0,0))+ theme_bw() +
theme(axis.ticks = element_blank(),legend.title = element_blank(),
#axis.text.x = element_text(angle = 45),
legend.position = c(0.75,0.25)) + scale_fill_colorblind()
ggplot(data = freq_dt) + ggtitle('Topics ordered by expected frequency')+
geom_bar(aes(x = rank,y = frequency,fill = ifelse(topic == c(11),'Topic 11',ifelse(topic==28,'Topic 28','Other'))),
stat = 'identity') +
geom_label_repel(aes(x = rank,y = frequency,label = topic))+
# coord_flip() +
scale_x_continuous(breaks = c(1:90),labels=freq_dt$topic,name = 'topic',expand=c(0,0))+ theme_bw() +
theme(axis.ticks = element_blank(),legend.title = element_blank(),
#axis.text.x = element_text(angle = 45),
legend.position = c(0.75,0.25)) + scale_fill_colorblind()
ggplot(data = freq_dt) + ggtitle('Topics ordered by expected frequency')+
geom_bar(aes(x = rank,y = frequency,fill = ifelse(topic == c(11),'Topic 11',ifelse(topic==28,'Topic 28','Other'))),
stat = 'identity') +
geom_label(aes(x = rank,y = frequency,label = topic))+
# coord_flip() +
scale_x_continuous(breaks = c(1:90),labels=freq_dt$topic,name = 'topic',expand=c(0,0))+ theme_bw() +
theme(axis.ticks = element_blank(),legend.title = element_blank(),
#axis.text.x = element_text(angle = 45),
legend.position = c(0.75,0.25)) + scale_fill_colorblind()
frequency <- colMeans(model.stm$theta)
freq_dt = data.table(frequency,rank = rank(frequency),topic = 1:90)
freq_dt = freq_dt[order(rank)]
ggplot(data = freq_dt) + ggtitle('Topics ordered by expected frequency')+
geom_bar(aes(x = rank,y = frequency,fill = ifelse(topic == c(11),'Topic 11',ifelse(topic==28,'Topic 28','Other'))),
stat = 'identity') +
geom_label(aes(x = rank,y = frequency,label = topic))+
# coord_flip() +
scale_x_continuous(breaks = c(1:90),labels=freq_dt$topic,name = 'topic',expand=c(0,0))+ theme_bw() +
theme(axis.ticks = element_blank(),legend.title = element_blank(),
#axis.text.x = element_text(angle = 45),
legend.position = c(0.75,0.25)) + scale_fill_colorblind()
freq_dt
frequency <- colMeans(model.stm$theta)
freq_dt = data.table(frequency,rank = rank(-frequency),topic = 1:90)
freq_dt = freq_dt[order(rank)]
ggplot(data = freq_dt) + ggtitle('Topics ordered by expected frequency')+
geom_bar(aes(x = rank,y = frequency,fill = ifelse(topic == c(11),'Topic 11',ifelse(topic==28,'Topic 28','Other'))),
stat = 'identity') +
geom_label(aes(x = rank,y = frequency,label = topic))+
coord_flip() +
scale_x_continuous(breaks = c(1:90),labels=freq_dt$topic,name = 'topic',expand=c(0,0))+ theme_bw() +
theme(axis.ticks = element_blank(),legend.title = element_blank(),
#axis.text.x = element_text(angle = 45),
legend.position = c(0.75,0.25)) + scale_fill_colorblind()
freq_dt
freq_dt[topic%in%c(11,28)]
# eff_all_un = stm::estimateEffect(stmobj = model.stm,formula = c(11,28)~YEAR + AGENCY + PROJECT_TOPIC +
#                                    PROJECT_TYPE + skills_rating*ideo_rating,
#                                  nsims = nsim,metadata = as.data.frame(dfm2stm$meta))
# saveRDS(eff_all_un,paste0(where_is_scratch,'effect_estimates_withuncertainty.RDS'))
eff_all_un = paste0(where_is_scratch,'effect_estimates_withuncertainty.RDS')
# eff_all_un = stm::estimateEffect(stmobj = model.stm,formula = c(11,28)~YEAR + AGENCY + PROJECT_TOPIC +
#                                    PROJECT_TYPE + skills_rating*ideo_rating,
#                                  nsims = nsim,metadata = as.data.frame(dfm2stm$meta))
# saveRDS(eff_all_un,paste0(where_is_scratch,'effect_estimates_withuncertainty.RDS'))
eff_all_un = readRDS(paste0(where_is_scratch,'effect_estimates_withuncertainty.RDS'))
topic_task_coefs = coefs[!grepl('Agency|Year',GROUP)&!grepl('Intercept|rating',coef),]
topic_task_coefs$topic_or_task = ifelse(grepl('Topic',topic_task_coefs$coef),'Topic','Task')
ggplot(topic_task_coefs) + #facet_wrap(~topic)+
geom_vline(xintercept = 0,lty = 2,col = 'grey80')+
geom_errorbarh(aes(xmin = Estimate-1.96*`Std. Error`,xmax = Estimate+1.96*`Std. Error`,
y= coef,col = topic),height = 0.2,position = position_dodge(0.3))+
geom_point(aes(x = Estimate,y = coef,col = topic),position = position_dodge(0.3)) +
scale_color_colorblind()+ scale_x_continuous(limits=c(-0.02,0.02))+
ylab('') + theme_bw() +
# facet_wrap(~topic_or_task)+
theme(axis.title.y = element_blank(),legend.position = c(0.75,0.25),axis.text = element_text(size = 12)) +
xlab('Estimate 95% confidence intervals') + ggtitle('Topic focus regressed on focus and task')
coef_sum = summary(eff_all_un)
coef11 = data.table(coef_sum$tables[[1]])
coef11$topic = '11: climate-->project'
coef11$coef = rownames(coef_sum$tables[[1]])
coef28 = data.table(coef_sum$tables[[2]])
coef28$topic = '28: project-->climate'
coef28$coef = rownames(coef_sum$tables[[2]])
coefs = rbind(coef11,coef28)
coefs$GROUP[grepl('YEAR',coefs$coef)]<-'Year'
coefs$GROUP[grepl('AGENCY',coefs$coef)]<-'Agency'
coefs$GROUP[grepl('TOPIC',coefs$coef)]<-'Topic'
coefs$GROUP[grepl('TYPE',coefs$coef)]<-'Task'
coefs$GROUP[grepl('skill|ideo',coefs$coef)]<-'Skill/Ideol.'
coefs$coef <- gsub('ideo','Ideology',coefs$coef)
coefs$coef <- gsub('skill','Workforce skill',coefs$coef)
coefs$coef <- gsub('PROJECT_TOPIC','Topic: ',coefs$coef)
coefs$coef <- gsub('PROJECT_TYPE','Task: ',coefs$coef)
coefs$coef <- gsub('YEAR','Year: ',coefs$coef)
coefs$coef <- gsub('AGENCY','',coefs$coef)
require(forcats)
require(ggthemes)
coefs$coef <- fct_inorder(coefs$coef)
coefs$coef <- fct_rev(coefs$coef)
topic_task_coefs = coefs[!grepl('Agency|Year',GROUP)&!grepl('Intercept|rating',coef),]
topic_task_coefs$topic_or_task = ifelse(grepl('Topic',topic_task_coefs$coef),'Topic','Task')
ggplot(topic_task_coefs) + #facet_wrap(~topic)+
geom_vline(xintercept = 0,lty = 2,col = 'grey80')+
geom_errorbarh(aes(xmin = Estimate-1.96*`Std. Error`,xmax = Estimate+1.96*`Std. Error`,
y= coef,col = topic),height = 0.2,position = position_dodge(0.3))+
geom_point(aes(x = Estimate,y = coef,col = topic),position = position_dodge(0.3)) +
scale_color_colorblind()+ scale_x_continuous(limits=c(-0.02,0.02))+
ylab('') + theme_bw() +
# facet_wrap(~topic_or_task)+
theme(axis.title.y = element_blank(),legend.position = c(0.75,0.25),axis.text = element_text(size = 12)) +
xlab('Estimate 95% confidence intervals') + ggtitle('Topic focus regressed on focus and task')
coef_sum = summary(eff_all_un)
coef11 = data.table(coef_sum$tables[[1]])
coef11$topic = '11: climate-->project'
coef11$coef = rownames(coef_sum$tables[[1]])
coef28 = data.table(coef_sum$tables[[2]])
coef28$topic = '28: project-->climate'
coef28$coef = rownames(coef_sum$tables[[2]])
coefs = rbind(coef11,coef28)
coefs$GROUP[grepl('YEAR',coefs$coef)]<-'Year'
coefs$GROUP[grepl('AGENCY',coefs$coef)]<-'Agency'
coefs$GROUP[grepl('TOPIC',coefs$coef)]<-'Focus'
coefs$GROUP[grepl('TYPE',coefs$coef)]<-'Task'
coefs$GROUP[grepl('skill|ideo',coefs$coef)]<-'Skill/Ideol.'
coefs$coef <- gsub('ideo','Ideology',coefs$coef)
coefs$coef <- gsub('skill','Workforce skill',coefs$coef)
coefs$coef <- gsub('PROJECT_TOPIC','Topic: ',coefs$coef)
coefs$coef <- gsub('PROJECT_TYPE','Task: ',coefs$coef)
coefs$coef <- gsub('YEAR','Year: ',coefs$coef)
coefs$coef <- gsub('AGENCY','',coefs$coef)
require(forcats)
require(ggthemes)
coefs$coef <- fct_inorder(coefs$coef)
coefs$coef <- fct_rev(coefs$coef)
topic_task_coefs = coefs[!grepl('Agency|Year',GROUP)&!grepl('Intercept|rating',coef),]
topic_task_coefs$topic_or_task = ifelse(grepl('Topic',topic_task_coefs$coef),'Topic','Task')
ggplot(topic_task_coefs) + #facet_wrap(~topic)+
geom_vline(xintercept = 0,lty = 2,col = 'grey80')+
geom_errorbarh(aes(xmin = Estimate-1.96*`Std. Error`,xmax = Estimate+1.96*`Std. Error`,
y= coef,col = topic),height = 0.2,position = position_dodge(0.3))+
geom_point(aes(x = Estimate,y = coef,col = topic),position = position_dodge(0.3)) +
scale_color_colorblind()+ scale_x_continuous(limits=c(-0.02,0.02))+
ylab('') + theme_bw() +
# facet_wrap(~topic_or_task)+
theme(axis.title.y = element_blank(),legend.position = c(0.75,0.25),axis.text = element_text(size = 12)) +
xlab('Estimate 95% confidence intervals') + ggtitle('Topic focus regressed on focus and task')
coef_sum = summary(eff_all_un)
coef11 = data.table(coef_sum$tables[[1]])
coef11$topic = '11: climate-->project'
coef11$coef = rownames(coef_sum$tables[[1]])
coef28 = data.table(coef_sum$tables[[2]])
coef28$topic = '28: project-->climate'
coef28$coef = rownames(coef_sum$tables[[2]])
coefs = rbind(coef11,coef28)
coefs$GROUP[grepl('YEAR',coefs$coef)]<-'Year'
coefs$GROUP[grepl('AGENCY',coefs$coef)]<-'Agency'
coefs$GROUP[grepl('TOPIC',coefs$coef)]<-'Focus'
coefs$GROUP[grepl('TYPE',coefs$coef)]<-'Task'
coefs$GROUP[grepl('skill|ideo',coefs$coef)]<-'Skill/Ideol.'
coefs$coef <- gsub('ideo','Ideology',coefs$coef)
coefs$coef <- gsub('skill','Workforce skill',coefs$coef)
coefs$coef <- gsub('PROJECT_TOPIC','Focus: ',coefs$coef)
coefs$coef <- gsub('PROJECT_TYPE','Task: ',coefs$coef)
coefs$coef <- gsub('YEAR','Year: ',coefs$coef)
coefs$coef <- gsub('AGENCY','',coefs$coef)
require(forcats)
require(ggthemes)
coefs$coef <- fct_inorder(coefs$coef)
coefs$coef <- fct_rev(coefs$coef)
topic_task_coefs = coefs[!grepl('Agency|Year',GROUP)&!grepl('Intercept|rating',coef),]
topic_task_coefs$topic_or_task = ifelse(grepl('Topic',topic_task_coefs$coef),'Topic','Task')
ggplot(topic_task_coefs) + #facet_wrap(~topic)+
geom_vline(xintercept = 0,lty = 2,col = 'grey80')+
geom_errorbarh(aes(xmin = Estimate-1.96*`Std. Error`,xmax = Estimate+1.96*`Std. Error`,
y= coef,col = topic),height = 0.2,position = position_dodge(0.3))+
geom_point(aes(x = Estimate,y = coef,col = topic),position = position_dodge(0.3)) +
scale_color_colorblind()+ scale_x_continuous(limits=c(-0.02,0.02))+
ylab('') + theme_bw() +
# facet_wrap(~topic_or_task)+
theme(axis.title.y = element_blank(),legend.position = c(0.75,0.25),axis.text = element_text(size = 12)) +
xlab('Estimate 95% confidence intervals') + ggtitle('Topic focus regressed on focus and task')
pack = c('data.table','stringr','tidyverse','doParallel','pdftools','lubridate')
need = pack[!pack %in% installed.packages()[,'Package']]
lapply(need,install.packages)
lapply(pack,require,character.only=T)
empty_project_record = data.table(PROJECT_ID = character(),YEAR = numeric(),PROJECT_TYPE = character(),AGENCY = character())
empty_doc_dt = data.table(YEAR = numeric(),FILE_NAME = character(), FILE_LOC = character(), PROJECT_TYPE = character(),AGENCY = character())
###############
###############
epa = fread('../eis_documents/enepa_repository/meta_data/eis_record_detail.csv')
epa = epa[Document=='Final',]
epa = epa[grepl('^201[3-9]',epa$EIS.Number),]
#epa = epa[Agency=='Bureau of Land Management',]
epa$Title = iconv(epa$Title,'utf8')
epa$Year = str_extract(epa$EIS.Number,'^[0-9]{4}')
epa  = epa[!grepl('ADOPTION|WITHDRAWN|^Withdrawn|^Adoption',Title)&!grepl('PRO',State.or.Territory),]
epa[EIS.Number%in% c('20170008','20170006'),]
epa[grepl('Gateway|Transwest',Title),]
epa[grepl('Energy Gateway|Transwest',Title),]
epa[grepl('Energy Gateway|Trans(W|w)est',Title),]
# library(tm)
# library(quanteda)
# library(data.table)
# library(textclean)
# library(stringr)
# library(textmineR)
# library(stm)
# library(textmineR)
# library(textreuse)
# library(text2vec)
# library(pbapply)
#setwd('bucket_mount/tuolumne/')
rerun_existing = T
if(!require(data.table)){install.packages('data.table');require(data.table)}
if(!require(tm)){install.packages('tm');require(tm)}
if(!require(quanteda)){install.packages('quanteda');require(quanteda)}
if(!require(textclean)){install.packages('textclean');require(textclean)}
if(!require(stringr)){install.packages('stringr');require(stringr)}
if(!require(pbapply)){install.packages('pbapply');require(pbapply)}
if(!require(parallel)){install.packages('parallel');require(parallel)}
if(!require(doParallel)){install.packages('doParallel');require(doParallel)}
if(!require(textreuse)){install.packages('textreuse');require(textreuse)}
#setwd("google_drive/tuolumne/")
#setDTthreads(3)
#getDTthreads()
mcores = detectCores() / 2
options("mc.cores" = detectCores() /2)
#table(projects$AGENCY,projects$PROJECT_TYPE)
projects = fread('scratch/boilerplate/project_candidates_eis_only.csv')
projects = projects[Document=='Final',]
projects = projects[grepl('^201[3-9]|^2020',PROJECT_ID),]
documents = fread( 'scratch/boilerplate/document_candidates_eis_only.csv')
documents = documents[PROJECT_ID %in% projects$PROJECT_ID,]
scratch_loc = 'scratch/boilerplate/hash_candidates/'
tokpars = 'scratch/tokenized_paragraphs/'
flist  = list.files(tokpars)
flist <- flist[grepl('^201[3-9]|^2020',flist)]
flist <- flist[!grepl('^[0-9]{8}_(CEQ|\\1)',)]
flist <- flist[flist %in% gsub('pdf$|PDF$','txt',documents$File_Name)]
flist = flist[!grepl('^([0-9]{8})_(CEQ|\\1)',flist)]
minhash <- minhash_generator(n = 240, seed = 40)
progress_bars = T
gc()
cluster = makeCluster(mcores)
registerDoParallel(cl = cluster,cores = mcores)
parallel::clusterEvalQ(cluster,'require(data.table)')
#####
hash_file = paste0(scratch_loc,'eis_page_hash_paragraph_candidates.RDS')
tlist = foreach(i = seq_along(flist)) %dopar% {
t1 = readLines(paste0(tokpars,flist[i]))
t1}
names(tlist)<- flist
full_tlist = unlist(tlist)
rm(tlist)
gc()
saveRDS(full_flist,'scratch/paragraph_list.rds')
saveRDS(full_tlist,'scratch/paragraph_list.rds')
